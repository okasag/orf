orf_model
summary(orf_model)
devtools::document()
devtools::check()
library(orf)
### 2. MAMMOGRAPHY DATA
load("~/Documents/HSG/ORF/data/mammography_dataset_rdata.Rdata")
# prepare data
dataset$SYMPT <- as.numeric(dataset$SYMPT)
dataset$PB <- as.numeric(dataset$PB)
dataset$HIST <- as.numeric(dataset$HIST)
dataset$BSE <- as.numeric(dataset$BSE)
dataset$DECT <- as.numeric(dataset$DECT)
dataset$y <- as.numeric(dataset$y)
# matrices
X <- as.matrix(dataset[, 1:ncol(dataset)-1])
Y <- as.matrix(dataset[, ncol(dataset)])
# prepare parameters
ntree <- 1000
mtry <- ceiling(sqrt(ncol(X)))
nmin <- 5
honesty <- TRUE
inference <- FALSE
margins <- FALSE
# estimate orf in first place
set.seed(311992) # put your birthday here
orf_model <- orf(X,Y,ntree,mtry,nmin,honesty,inference,margins)
summary(orf_model)
summary(orf_model, latex = 1)
summary(orf_model, latex = 9)
summary(orf_model, latex = TRUE)
devtools::document()
devtools::check()
library(orf)
### 2. MAMMOGRAPHY DATA
load("~/Documents/HSG/ORF/data/mammography_dataset_rdata.Rdata")
# prepare data
dataset$SYMPT <- as.numeric(dataset$SYMPT)
dataset$PB <- as.numeric(dataset$PB)
dataset$HIST <- as.numeric(dataset$HIST)
dataset$BSE <- as.numeric(dataset$BSE)
dataset$DECT <- as.numeric(dataset$DECT)
dataset$y <- as.numeric(dataset$y)
# matrices
X <- as.matrix(dataset[, 1:ncol(dataset)-1])
Y <- as.matrix(dataset[, ncol(dataset)])
# prepare parameters
ntree <- 1000
mtry <- ceiling(sqrt(ncol(X)))
nmin <- 5
honesty <- TRUE
inference <- FALSE
margins <- FALSE
# estimate orf in first place
set.seed(311992) # put your birthday here
orf_model <- orf(X,Y,ntree,mtry,nmin,honesty,inference,margins)
orf_model$forestInfo$inputs$inference <- TRUE
orf_model
summary(orf_model)
### 2. MAMMOGRAPHY DATA
load("~/Documents/HSG/ORF/data/mammography_dataset_rdata.Rdata")
dataset$y <- as.numeric(dataset$y)
# matrices
X <- as.matrix(dataset[, 1:ncol(dataset)-1])
Y <- as.matrix(dataset[, ncol(dataset)])
# prepare parameters
ntree <- 1000
mtry <- ceiling(sqrt(ncol(X)))
nmin <- 5
honesty <- TRUE
inference <- FALSE
margins <- FALSE
# estimate orf in first place
set.seed(311992) # put your birthday here
orf_model <- orf(X,Y,ntree,mtry,nmin,honesty,inference,margins)
### 2. MAMMOGRAPHY DATA
load("~/Documents/HSG/ORF/data/mammography_dataset_rdata.Rdata")
# prepare data
dataset$SYMPT <- as.numeric(dataset$SYMPT)
dataset$PB <- as.numeric(dataset$PB)
dataset$HIST <- as.numeric(dataset$HIST)
dataset$BSE <- as.numeric(dataset$BSE)
dataset$DECT <- as.numeric(dataset$DECT)
dataset$y <- as.numeric(dataset$y)
### 2. MAMMOGRAPHY DATA
load("~/Documents/HSG/ORF/data/mammography_dataset_rdata.Rdata")
# prepare data
dataset$SYMPT <- as.numeric(dataset$SYMPT)
dataset$PB <- as.numeric(dataset$PB)
dataset$HIST <- as.numeric(dataset$HIST)
dataset$BSE <- as.numeric(dataset$BSE)
dataset$DECT <- as.numeric(dataset$DECT)
# matrices
X <- as.matrix(dataset[, 1:ncol(dataset)-1])
Y <- as.matrix(dataset[, ncol(dataset)])
# prepare parameters
ntree <- 1000
mtry <- ceiling(sqrt(ncol(X)))
nmin <- 5
honesty <- TRUE
inference <- FALSE
margins <- FALSE
# estimate orf in first place
set.seed(311992) # put your birthday here
orf_model <- orf(X,Y,ntree,mtry,nmin,honesty,inference,margins)
### 2. MAMMOGRAPHY DATA
load("~/Documents/HSG/ORF/data/mammography_dataset_rdata.Rdata")
# prepare data
dataset$SYMPT <- as.numeric(dataset$SYMPT)
dataset$PB <- as.numeric(dataset$PB)
dataset$HIST <- as.numeric(dataset$HIST)
dataset$BSE <- as.numeric(dataset$BSE)
dataset$DECT <- as.numeric(dataset$DECT)
dataset$y <- as.numeric(dataset$y)
# matrices
X <- as.matrix(dataset[, 1:ncol(dataset)-1])
Y <- (dataset[, ncol(dataset)]
)
# prepare parameters
ntree <- 1000
mtry <- ceiling(sqrt(ncol(X)))
nmin <- 5
honesty <- TRUE
inference <- FALSE
margins <- FALSE
# estimate orf in first place
set.seed(311992) # put your birthday here
orf_model <- orf(X,Y,ntree,mtry,nmin,honesty,inference,margins)
orf_model
summary(orf_model)
# load orf
library(orf)
# estimate with polr
library(MASS)
library(erer)
### 2. MAMMOGRAPHY DATA
load("~/Documents/HSG/ORF/data/mammography_dataset_rdata.Rdata")
# prepare data
dataset$SYMPT <- as.numeric(dataset$SYMPT)
dataset$PB <- as.numeric(dataset$PB)
dataset$HIST <- as.numeric(dataset$HIST)
dataset$BSE <- as.numeric(dataset$BSE)
dataset$DECT <- as.numeric(dataset$DECT)
dataset$y <- as.numeric(dataset$y)
# matrices
X <- as.matrix(dataset[, 1:ncol(dataset)-1])
Y <- as.matrix(dataset[, ncol(dataset)])
# prepare parameters
ntree <- 1000
mtry <- ceiling(sqrt(ncol(X)))
nmin <- 5
honesty <- TRUE
inference <- FALSE
margins <- FALSE
# estimate orf in first place
set.seed(311992) # put your birthday here
orf_model <- orf(X,Y,ntree,mtry,nmin,honesty,inference,margins)
orf_model
orf_model$honestMSE
orf_model$honestPredictions
summary(orf_model, latex = TRUE)
summary(orf_model, latex = FALSE)
plot(orf_model)
1.200322/0.533666
0.09/0.1
0.08/0.09
sqrt(200)
sqrt(800)
sqrt(3200)
14*0.2
28/0.2
28*0.2
14*0.4
14*0.1
28*0.05
56*0.05
56*0.025
28*0.09
56*0.08
sqrt(0.1)
sqrt(0.09)
sqrt(0.08)
sqrt(0.1)
sqrt(0.1)/2
sqrt(0.1)/4
wd
wd()
getwd()
library(devtools)
check()
build()
load_all()
### 2. MAMMOGRAPHY DATA
load("~/Documents/HSG/ORF/data/mammography_dataset_rdata.Rdata")
# prepare data
dataset$SYMPT <- as.numeric(dataset$SYMPT)
dataset$PB <- as.numeric(dataset$PB)
dataset$HIST <- as.numeric(dataset$HIST)
dataset$BSE <- as.numeric(dataset$BSE)
dataset$DECT <- as.numeric(dataset$DECT)
dataset$y <- as.numeric(dataset$y)
# matrices
X <- as.matrix(dataset[, 1:ncol(dataset)-1])
Y <- as.matrix(dataset[, ncol(dataset)])
# prepare parameters
ntree <- 1000
mtry <- ceiling(sqrt(ncol(X)))
nmin <- 5
honesty <- TRUE
inference <- FALSE
margins <- FALSE
# estimate orf in first place
set.seed(311992) # put your birthday here
mrf_model <- mrf(X,Y,ntree,mtry,nmin,honesty,inference,margins)
class(mrf_model)
mrf_model$trainForests
mrf_model$forestInfo
mrf_model$honestPredictions
mrf_model$predictedCategories
mrf_model$honestMSE
mrf_model$honestRPS
forest <- mrf_model
newdata = NULL
eval = "atmean"
### decide if prediction or in sample marginal effects should be evaluated
if (is.null(newdata)) {
# if no newdata supplied, estimate in sample marginal effects
if (forest$forestInfo$inputs$honesty == FALSE) {
data <- forest$forestInfo$trainData # take in-sample data
} else if (forest$forestInfo$inputs$honesty == TRUE) {
data <- forest$forestInfo$honestData
}
} else {
# check if newdata is compatible with train data
if (ncol(newdata) != ncol(forest$forestInfo$trainData)) {
stop("newdata is not compatible with training data. Programme terminated.")
} else {
data = newdata
}
}
data
all.equal(data, mrf_model$forestInfo$honestData)
### data preparation and checks
# get number of observations
n_data <- as.numeric(nrow(data))
n_data
# get categories
categories <- forest$forestInfo$categories
categories
# get X as matrix
X <- as.matrix(data[, -1])
# get Y as matrix
Y <- as.matrix(data[, 1])
Y
X
# create indicator variables (outcomes)
Y_ind <- lapply(categories[1:length(categories)-1], function(x) ifelse((Y <= x), 1, 0))
Y_ind
# create indicator variables (outcomes) now with equality for each single category
Y_ind <- lapply(categories, function(x) ifelse((Y == x), 1, 0))
# create datasets with indicator outcomes
data_ind <- lapply(Y_ind, function(x) as.data.frame(cbind(as.matrix(unlist(x)), X)))
View(data_ind)
### marginal effects preparation
# share of SD to be used
h_std <- 0.1
# check if X is continuous or dummy or categorical
X_type <- apply(X, 2, function(x) length(unique(x)))
X_type
# now determine the type of X
X_continuous <- which(X_type > 10) # define IDs of continuous Xs
X_dummy <- which(X_type == 2) # define IDs of dummies
X_categorical <- which(X_type > 2 & X_type <= 10)
X_continuous
X_dummy
X_categorical
# additional check for constant variables which are nonsensical
if (any(X_type == 1) | any(X_type == 0)) {
stop("Some of the covariates are constant. This makes no sense for evaluation of marginal effects. Programme terminated.")
}
### check the evaluation point
if (eval == "atmean") {
# variable of interest: X_1 to X_last, ME at mean
X_mean <- lapply(1:ncol(X), function(x) t(as.matrix(colMeans(X)))) # set all Xs to their mean values (so many times as we have Xs)
} else if (eval == "atmedian") {
# variable of interest: X_1 to X_last, ME at median
X_mean <- lapply(1:ncol(X), function(x) t(as.matrix(apply(X, 2, median)))) # set all Xs to their median values (so many times as we have Xs)
} else if (eval == "mean") {
# # variable of interest: X_1 to X_last, mean ME
X_mean <- lapply(1:ncol(X), function(x) X) # set all Xs to their exact values (so many times as we have Xs)
} else {
stop("Incorrect evaluation point. This must be one of be one of mean, atmean, or atmedian. Programme terminated.")
}
### get data needed for evaluation of ME
# get number of evaluation points
X_rows <- nrow(X_mean[[1]])
# get number of Xs
X_cols <- ncol(X_mean[[1]])
# get SD of Xs
X_sd <- rep_row(apply(X, 2, sd), n = X_rows)
# create X_up (X_mean + 0.1 * X_sd)
X_up <- X_mean[[1]] + h_std*X_sd
# create X_down (X_mean - 0.1 * X_sd)
X_down <- X_mean[[1]] - h_std*X_sd
X_up
X_down
## now check for the support of X
# check X_max
X_max <- rep_row(apply(X, 2, max), n = X_rows)
# check X_min
X_min <- rep_row(apply(X, 2, min), n = X_rows)
# check if X_up is within the range X_min and X_max
X_up <- (X_up < X_max) * X_up + (X_up >= X_max) * X_max
X_up <- (X_up > X_min) * X_up + (X_up <= X_min) * (X_min + h_std * X_sd)
# check if X_down is within the range X_min and X_max
X_down <- (X_down > X_min) * X_down + (X_down <= X_min) * X_min
X_down <- (X_down < X_max) * X_down + (X_down >= X_max) * (X_max - h_std * X_sd)
# check if X_up and X_down are same
if (any(X_up == X_down)) {
# adjust to higher share of SD
X_up   <- (X_up > X_down) * X_up   + (X_up == X_down) * (X_up   + 0.5 * h_std * X_sd)
X_down <- (X_up > X_down) * X_down + (X_up == X_down) * (X_down - 0.5 * h_std * X_sd)
# check the min max range again
X_up   <- (X_up < X_max) * X_up + (X_up >= X_max) * X_max
X_down <- (X_down > X_min) * X_down + (X_down <= X_min) * X_min
}
## now we need 2 datasets: one with X_up and second with X_down
# X_mean_up all (continous)
X_mean_up <- X_mean
X_mean_down <- X_mean
# replace values accordingly
for (i in 1:X_cols) {
X_mean_up[[i]][, i] <- X_up[, i]
X_mean_down[[i]][, i] <- X_down[, i]
}
# adjust for categorical X (works also for zero categorical)
for (i in X_categorical) {
X_mean_up[[i]][, i] <- ceiling(mean(X[, i]))
X_mean_down[[i]][, i] <- floor(mean(X[, i]))
}
# adjust for dummies (works also for zero dummies)
for (i in X_dummy) {
X_mean_up[[i]][, i] <- max(X[, i])
X_mean_down[[i]][, i] <- min(X[, i])
}
X_mean_up
X_mean_down
forest$forestInfo$inputs$honesty
forest$forestInfo$inputs$inference == FALSE
#### now we do not need weights if we do not need inference (based on out of bag predictions)
# forest prediction for X_mean_up (mean doesnt matter for atmean or atmedian)
forest_pred_up <- lapply(forest$trainForests, function(x) lapply(X_mean_up, function(y) mean(predict(x, data = y)$predictions)))
# forest prediction for X_mean_down (mean doesnt matter for atmean or atmedian)
forest_pred_down <- lapply(forest$trainForests, function(x) lapply(X_mean_down, function(y) mean(predict(x, data = y)$predictions)))
forest$trainForests
forest_pred_up
forest_pred_down
# do honest predictions
# forest prediction for X_mean_up (use new faster function particularly for ME)
forest_pred_up <- predict_forest_preds_for_ME(forest$trainForests, data_ind, X_mean_up)
# forest prediction for X_mean_down
forest_pred_down <- predict_forest_preds_for_ME(forest$trainForests, data_ind, X_mean_down)
forest_pred_up
forest_pred_down
forest_pred_down_1 <- forest_pred_down
# forest prediction for X_mean_down
forest_pred_down <- predict_forest_preds_for_ME(forest$trainForests, data_ind, X_mean_down)
all.equal(forest_pred_down_1, forest_pred_down)
# do honest predictions with weight based inference
# extract weights for desired Xs up: get weights from honest sample and predict weights for evaluation points from HONEST sample
forest_weights_up <- predict_forest_weights_for_ME(forest$trainForests, X, X_mean_up)
# extract weights for desired Xs down
forest_weights_down <- predict_forest_weights_for_ME(forest$trainForests, X, X_mean_down)
## compute predictions based on weights
# forest prediction for X_mean_up
forest_pred_up <- mapply(function(x,y) lapply(x, function(x) as.numeric(x%*%y)), forest_weights_up, Y_ind, SIMPLIFY = FALSE)
# forest prediction for X_mean_down
forest_pred_down <- mapply(function(x,y) lapply(x, function(x) as.numeric(x%*%y)), forest_weights_down, Y_ind, SIMPLIFY = FALSE)
forest_pred_down
forest_pred_down_1
all.equal(forest_pred_down_1, forest_pred_down)
### form MRF predictions
# prepare up
forest_pred_up_1 <- append(forest_pred_up, list(rep(list(rep(1, 1)), X_cols))) # append 1
forest_pred_up_0 <- append(list(rep(list(rep(0, 1)), X_cols)), forest_pred_up) # prepend 0
forest_pred_up_0
# isolate predictions
forest_pred_up <- mapply(function(x,y) mapply(function(x,y) x-y, x, y,  SIMPLIFY = F), forest_pred_up_1, forest_pred_up_0, SIMPLIFY = F)
# avoid negative predictions
forest_pred_up <- lapply(forest_pred_up, function(x) lapply(x, function(x) ifelse((x < 0), 0, x)))
# normalize predictions
forest_pred_up_rowsum <- lapply(seq_along(forest_pred_up[[1]]), function(i) rowSums(matrix(sapply(forest_pred_up, "[[", i), ncol = length(categories), nrow = 1))) # build rowsums with respect to categories
forest_pred_up <- lapply(forest_pred_up, function(x) mapply(function(x,y) x/y, x, forest_pred_up_rowsum, SIMPLIFY = FALSE)) # normalize to sum up to 1 (very rare but just to be sure)
### form MRF predictions
## now we have to normalize the predictions for MRF
# get rowsum for each X
# get rowsum for each X
forest_pred_up_rowsum <- lapply(seq_along(forest_pred_up[[1]]), function(k) sum(unlist(lapply(seq_along(forest_pred_up[[1]]), function(i) lapply(seq_along(forest_pred_up), function(j) unlist(forest_pred_up[[j]][[i]])))[[k]])))
forest_pred_up_rowsum
lapply(seq_along(forest_pred_up[[1]]), function(i) rowSums(matrix(sapply(forest_pred_up, "[[", i), ncol = length(categories), nrow = 1)))
forest_pred_up
## compute predictions based on weights
# forest prediction for X_mean_up
forest_pred_up <- mapply(function(x,y) lapply(x, function(x) as.numeric(x%*%y)), forest_weights_up, Y_ind, SIMPLIFY = FALSE)
# forest prediction for X_mean_down
forest_pred_down <- mapply(function(x,y) lapply(x, function(x) as.numeric(x%*%y)), forest_weights_down, Y_ind, SIMPLIFY = FALSE)
forest_pred_up
# forest prediction for X_mean_down
forest_pred_down <- mapply(function(x,y) lapply(x, function(x) as.numeric(x%*%y)), forest_weights_down, Y_ind, SIMPLIFY = FALSE)
lapply(seq_along(forest_pred_up[[1]]), function(k) sum(unlist(lapply(seq_along(forest_pred_up[[1]]), function(i) lapply(seq_along(forest_pred_up), function(j) unlist(forest_pred_up[[j]][[i]])))[[k]])))
lapply(seq_along(forest_pred_up[[1]]), function(i) rowSums(matrix(sapply(forest_pred_up, "[[", i), ncol = length(categories), nrow = 1)))
forest_pred_up_rowsum
### form MRF predictions
## now we have to normalize the predictions for MRF
# get rowsum for each X
forest_pred_up_rowsum <- lapply(seq_along(forest_pred_up[[1]]), function(i) rowSums(matrix(sapply(forest_pred_up, "[[", i), ncol = length(categories), nrow = 1))) # build rowsums with respect to categories
forest_pred_up_rowsum
# normalize, i.e. divide each element by the sum of elements (for each X row)
forest_pred_up_norm <- lapply(forest_pred_up, function(x) mapply(function(x,y) x/y, x, forest_pred_up_rowsum, SIMPLIFY = FALSE)) # normalize to sum up to 1 (very rare but just to be sure)
forest_pred_up_norm
## now we have to normalize the predictions for MRF X_down
# get rowsum for each X
forest_pred_down_rowsum <- lapply(seq_along(forest_pred_down[[1]]), function(i) rowSums(matrix(sapply(forest_pred_down, "[[", i), ncol = length(categories), nrow = 1))) # build rowsums with respect to categories
# normalize, i.e. divide each element by the sum of elements (for each X row)
forest_pred_down_norm <- lapply(forest_pred_down, function(x) mapply(function(x,y) x/y, x, forest_pred_down_rowsum, SIMPLIFY = FALSE)) # normalize to sum up to 1 (very rare but just to be sure)
forest_pred_down_rowsum
forest_pred_down_norm
forest_pred_diff_up_down
### now subtract the predictions according to the ME formula
forest_pred_diff_up_down <- mapply(function(x,y) mapply(function(x,y) x-y, x, y,  SIMPLIFY = F), forest_pred_up_norm, forest_pred_down_norm, SIMPLIFY = F)
forest_pred_diff_up_down
# compute the scaling factor: X_up-X_down=2*X_sd
scaling_factor <- lapply(1:X_cols, function(i) mean(as.numeric((X_up - X_down)[, i]))) # save it as separate list vectors (mean doesnt change anything for "atmean" option)
scaling_factor
as.list(X_up - X_down)
# set scaling factor to zero for categorical and dummy variables
for (i in (union(X_categorical, X_dummy))) {
scaling_factor[[i]] <- 1
}
scaling_factor
### variance for the marginal effects
## compute prerequisities for variance of honest marginal effects
# mean of scaling factor and squared afterwards (for atmean and atmedian the averaging doesnt change anything)
scaling_factor_squared <- lapply(scaling_factor, function(x) (mean(x))^2)
scaling_factor_squared
# now subtract the weights according to the ME formula
forest_weights_diff_up_down <- mapply(function(x,y) mapply(function(x,y) x-y, x, y,  SIMPLIFY = F), forest_weights_up, forest_weights_down, SIMPLIFY = F)
forest_weights_diff_up_down
n_data
lapply(forest_pred_diff_up_down, function(x) lapply(x, function(x) x/n_data))
forest_weights_up
forest_weights_diff_up_down
forest_weights_diff_up_down
# compute the conditional means: 1/N(weights%*%y) (predictions are based on honest sample)
forest_cond_means <- mapply(function(x,y) lapply(x, function(x) (x%*%y)/nrow(Y_ind[[1]])), forest_weights_diff_up_down, Y_ind, SIMPLIFY = FALSE)
forest_cond_means
forest_cond_means_mean <- lapply(forest_pred_diff_up_down, function(x) lapply(x, function(x) x/n_data))
forest_cond_means_mean
-0.001103467*206
forest_cond_means
forest_pred_diff_up_down
-0.001382608
# compute the conditional means: 1/N(weights%*%y) (predictions are based on honest sample)
forest_cond_means <- mapply(function(x,y) lapply(x, function(x) (x%*%y)/nrow(Y_ind[[1]])), forest_weights_diff_up_down, Y_ind, SIMPLIFY = FALSE)
forest_cond_means
forest_pred_diff_up_down
forest_pred_diff_up_down
forest_cond_means
mapply(function(x,y) lapply(x, function(x) (x%*%y), forest_weights_diff_up_down, Y_ind, SIMPLIFY = FALSE)
)
)
mapply(function(x,y) lapply(x, function(x) (x%*%y), forest_weights_diff_up_down, Y_ind, SIMPLIFY = FALSE))
# compute the conditional means: 1/N(weights%*%y) (predictions are based on honest sample)
forest_cond_means <- mapply(function(x,y) lapply(x, function(x) (x%*%y)/nrow(Y_ind[[1]])), forest_weights_diff_up_down, Y_ind, SIMPLIFY = FALSE)
forest_cond_means
mapply(function(x,y) lapply(x, function(x) (x%*%y)/nrow(Y_ind[[1]])), forest_weights_diff_up_down, Y_ind, SIMPLIFY = FALSE)
mapply(function(x,y) lapply(x, function(x) (x%*%y))), forest_weights_diff_up_down, Y_ind, SIMPLIFY = FALSE)
mapply(function(x,y) lapply(x, function(x) (x%*%y)), forest_weights_diff_up_down, Y_ind, SIMPLIFY = FALSE)
forest_pred_diff_up_down
# compute the conditional means: 1/N(weights%*%y) (predictions are based on honest sample)
forest_cond_means <- mapply(function(x,y) lapply(x, function(x) (x%*%y)/nrow(Y_ind[[1]])), forest_weights_diff_up_down, Y_ind, SIMPLIFY = FALSE)
# compute standard multiplication
forest_multi <- mapply(function(x,y) lapply(x, function(x) t(x)*y), forest_weights_diff_up_down, Y_ind, SIMPLIFY = FALSE)
forest_multi
# subtract the mean from each obs i
forest_multi_demeaned <- mapply(function(x,y) mapply(function(x,y) x-matrix(y, nrow = nrow(x)), x, y, SIMPLIFY = FALSE), forest_multi, forest_cond_means, SIMPLIFY = F)
forest_multi_demeaned
## now do the single variances for each category m
# square the demeaned and sum it and normalize
forest_multi_demeaned_sq_sum_norm <- lapply(forest_multi_demeaned, function(x) lapply(x, function(x) (sum(x^2))*(nrow(Y_ind[[1]])/(nrow(Y_ind[[1]])-1))))
forest_multi_demeaned_sq_sum_norm
# compute the conditional means: 1/N(weights%*%y) (predictions are based on honest sample)
forest_cond_means <- mapply(function(x,y) lapply(x, function(x) (x%*%y)/nrow(Y_ind[[1]])), forest_weights_diff_up_down, Y_ind, SIMPLIFY = FALSE)
## now do the single variances for each category m
# square the demeaned
forest_multi_demeaned_sq <- lapply(forest_multi_demeaned, function(x) lapply(x, function(x) x^2))
# sum all obs i together
forest_multi_demeaned_sq_sum <- lapply(forest_multi_demeaned_sq, function(x) lapply(x, function(x) sum(x)))
# divide by scaling factor
forest_multi_demeaned_sq_sum_scaled <- lapply(forest_multi_demeaned_sq_sum, function(x) mapply(function(x,y) x/y, x, scaling_factor_squared, SIMPLIFY = FALSE) )
# multiply by N/N-1 (normalize)
forest_multi_demeaned_sq_sum_scaled_norm <- lapply(forest_multi_demeaned_sq_sum_scaled, function(x) lapply(x, function(x) x*(n_data/(n_data-1)) ))
forest_multi_demeaned_sq_sum_scaled_norm
lapply(forest_multi_demeaned, function(x) lapply(x, function(x) (sum(x^2))*(nrow(Y_ind[[1]])/(nrow(Y_ind[[1]])-1))))
## now do the single variances for each category m
# square the demeaned and sum it and normalize (# square the demeaned, # sum all obs i together, # multiply by N/N-1 (normalize))
forest_multi_demeaned_sq_sum_norm <- lapply(forest_multi_demeaned, function(x) lapply(x, function(x) (sum(x^2))*(nrow(Y_ind[[1]])/(nrow(Y_ind[[1]])-1))))
# divide by scaling factor to get the variance
variance <- lapply(forest_multi_demeaned_sq_sum_norm, function(x) mapply(function(x,y) x/y, x, scaling_factor_squared, SIMPLIFY = FALSE) )
devtools::document()
devtools::check()
library(orf)
