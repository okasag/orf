for (i in 1:X_cols) {
X_mean_up[[i]][, i] <- X_up[, i]
X_mean_down[[i]][, i] <- X_down[, i]
}
# adjust for categorical X (works also for zero categorical) (adjustment such that the difference is always 1)
for (i in X_categorical) {
X_mean_up[[i]][, i] <- ceiling(X_mean_up[[i]][, i])
X_mean_down[[i]][, i] <- ifelse(ceiling(X_mean_down[[i]][, i]) == ceiling(X_mean_up[[i]][, i]),
floor(X_mean_down[[i]][, i]),
ceiling(X_mean_down[[i]][, i])
)
}
# adjust for dummies (works also for zero dummies)
for (i in X_dummy) {
X_mean_up[[i]][, i] <- max(X[, i])
X_mean_down[[i]][, i] <- min(X[, i])
}
# ----------------------------------------------------------------------------------- #
### check honesty and inference
if (forest_honesty == FALSE & inference == FALSE) {
#### now we do not need weights if we do not need inference (based on out of bag predictions)
# forest prediction for X_mean_up (mean doesnt matter for atmean or atmedian)
forest_pred_up <- lapply(forest$trainForests, function(x) lapply(X_mean_up, function(y) mean(predict(x, data = y)$predictions)))
# forest prediction for X_mean_down (mean doesnt matter for atmean or atmedian)
forest_pred_down <- lapply(forest$trainForests, function(x) lapply(X_mean_down, function(y) mean(predict(x, data = y)$predictions)))
} else if (forest_honesty == TRUE & inference == FALSE) {
# do honest predictions
# forest prediction for X_mean_up (use new faster function particularly for ME)
forest_pred_up <- predict_forest_preds_for_ME(forest$trainForests, data_ind, X_mean_up)
# forest prediction for X_mean_down
forest_pred_down <- predict_forest_preds_for_ME(forest$trainForests, data_ind, X_mean_down)
} else if (forest_honesty == TRUE & inference == TRUE) {
# do honest predictions with weight based inference
# extract weights for desired Xs up: get weights from honest sample and predict weights for evaluation points from HONEST sample
forest_weights_up <- predict_forest_weights_for_ME(forest$trainForests, X, X_mean_up)
# extract weights for desired Xs down
forest_weights_down <- predict_forest_weights_for_ME(forest$trainForests, X, X_mean_down)
## compute predictions based on weights
# forest prediction for X_mean_up
forest_pred_up <- mapply(function(x,y) lapply(x, function(x) as.numeric(x%*%y)), forest_weights_up, Y_ind, SIMPLIFY = FALSE)
# forest prediction for X_mean_down
forest_pred_down <- mapply(function(x,y) lapply(x, function(x) as.numeric(x%*%y)), forest_weights_down, Y_ind, SIMPLIFY = FALSE)
}
# ----------------------------------------------------------------------------------- #
### form ORF predictions
# prepare up
forest_pred_up_1 <- append(forest_pred_up, list(rep(list(rep(1, 1)), X_cols))) # append 1
forest_pred_up_0 <- append(list(rep(list(rep(0, 1)), X_cols)), forest_pred_up) # prepend 0
# isolate predictions
forest_pred_up <- mapply(function(x,y) mapply(function(x,y) x-y, x, y,  SIMPLIFY = F), forest_pred_up_1, forest_pred_up_0, SIMPLIFY = F)
# avoid negative predictions
forest_pred_up <- lapply(forest_pred_up, function(x) lapply(x, function(x) ifelse((x < 0), 0, x)))
# normalize predictions
forest_pred_up_rowsum <- lapply(seq_along(forest_pred_up[[1]]), function(i) rowSums(matrix(sapply(forest_pred_up, "[[", i), ncol = length(categories), nrow = 1))) # build rowsums with respect to categories
forest_pred_up <- lapply(forest_pred_up, function(x) mapply(function(x,y) x/y, x, forest_pred_up_rowsum, SIMPLIFY = FALSE)) # normalize to sum up to 1 (very rare but just to be sure)
# prepare down
forest_pred_down_1 <- append(forest_pred_down, list(rep(list(rep(1, 1)), X_cols))) # append 1
forest_pred_down_0 <- append(list(rep(list(rep(0, 1)), X_cols)), forest_pred_down) # prepend 0
# isolate predictions
forest_pred_down <- mapply(function(x,y) mapply(function(x,y) x-y, x, y,  SIMPLIFY = F), forest_pred_down_1, forest_pred_down_0, SIMPLIFY = F)
# avoid negative predictions
forest_pred_down <- lapply(forest_pred_down, function(x) lapply(x, function(x) ifelse((x < 0), 0, x)))
# normalize predictions
forest_pred_down_rowsum <- lapply(seq_along(forest_pred_down[[1]]), function(i) rowSums(matrix(sapply(forest_pred_down, "[[", i), ncol = length(categories), nrow = 1))) # build rowsums with respect to categories
forest_pred_down <- lapply(forest_pred_down, function(x) mapply(function(x,y) x/y, x, forest_pred_down_rowsum, SIMPLIFY = FALSE)) # normalize to sum up to 1 (very rare but just to be sure)
# ----------------------------------------------------------------------------------- #
### now subtract the predictions according to the ME formula
forest_pred_diff_up_down <- mapply(function(x,y) mapply(function(x,y) x-y, x, y,  SIMPLIFY = F), forest_pred_up, forest_pred_down, SIMPLIFY = F)
# compute the scaling factor: X_up-X_down=2*X_sd
scaling_factor <- lapply(1:X_cols, function(i) mean(as.numeric((X_up - X_down)[, i]))) # save it as separate list vectors (mean doesnt change anything for "atmean" option)
# set scaling factor to zero for categorical and dummy variables
for (i in (union(X_categorical, X_dummy))) {
scaling_factor[[i]] <- 1
}
# scale the differences to get marginal effects
marginal_effects_scaled <- lapply(forest_pred_diff_up_down, function(x) mapply(function(x,y) x/y, x, scaling_factor, SIMPLIFY = FALSE) )
# ----------------------------------------------------------------------------------- #
# coerce to a matrix
marginal_effects <- sapply(marginal_effects_scaled, function(x) sapply(x, function(x) as.matrix(x)))
# add names
colnames(marginal_effects) <- sapply(categories, function(x) paste("Category", x, sep = " "))
rownames(marginal_effects) <- colnames(X)
# ----------------------------------------------------------------------------------- #
if (inference == TRUE) {
### variance for the marginal effects
## compute prerequisities for variance of honest marginal effects
# mean of scaling factor and squared afterwards (for atmean and atmedian the averaging doesnt change anything)
scaling_factor_squared <- lapply(scaling_factor, function(x) (mean(x))^2)
# now subtract the weights according to the ME formula
forest_weights_diff_up_down <- mapply(function(x,y) mapply(function(x,y) x-y, x, y,  SIMPLIFY = F), forest_weights_up, forest_weights_down, SIMPLIFY = F)
# compute the conditional means: 1/N(weights%*%y) (predictions are based on honest sample)
forest_cond_means <- mapply(function(x,y) lapply(x, function(x) (x%*%y)/nrow(Y_ind[[1]])), forest_weights_diff_up_down, Y_ind, SIMPLIFY = FALSE)
# compute standard multiplication
forest_multi <- mapply(function(x,y) lapply(x, function(x) t(x)*y), forest_weights_diff_up_down, Y_ind, SIMPLIFY = FALSE)
# subtract the mean from each obs i
forest_multi_demeaned <- mapply(function(x,y) mapply(function(x,y) x-matrix(y, nrow = nrow(x)), x, y, SIMPLIFY = FALSE), forest_multi, forest_cond_means, SIMPLIFY = F)
## now do the single variances for each category m
# square the demeaned and sum it and normalize
forest_multi_demeaned_sq_sum_norm <- lapply(forest_multi_demeaned, function(x) lapply(x, function(x) (sum(x^2))*(nrow(Y_ind[[1]])/(nrow(Y_ind[[1]])-1))))
## now compute the prerequisites for covariances
# multiply forest_var_multi_demeaned according to formula for covariance (shifted categories needed for computational convenience)
forest_multi_demeaned_0_last <- append(forest_multi_demeaned, list(rep(list(matrix(0, ncol = ncol(forest_multi_demeaned[[1]][[1]]), nrow = nrow(forest_multi_demeaned[[1]][[1]]))), ncol(X_eval)))) # append zero matrix list
forest_multi_demeaned_0_first <- append(list(rep(list(matrix(0, ncol = ncol(forest_multi_demeaned[[1]][[1]]), nrow = nrow(forest_multi_demeaned[[1]][[1]]))), ncol(X_eval))), forest_multi_demeaned) # prepend zero matrix list
# compute the multiplication of category m with m-1 according to the covariance formula (sum, normalize and multiply by 2)
forest_multi_demeaned_cov_sum_norm_mult2 <- mapply(function(x,y) mapply(function(x,y) (sum(x*y))*(nrow(Y_ind[[1]])/(nrow(Y_ind[[1]])-1))*2, x, y, SIMPLIFY = FALSE), forest_multi_demeaned_0_first, forest_multi_demeaned_0_last, SIMPLIFY = F)
# divide by scaling factor to get the variance
variance <- lapply(forest_multi_demeaned_sq_sum_norm, function(x) mapply(function(x,y) x/y, x, scaling_factor_squared, SIMPLIFY = FALSE) )
## single variances done
# ----------------------------------------------------------------------------------- #
# divide by scaling factor
covariance <- lapply(forest_multi_demeaned_cov_sum_norm_mult2, function(x) mapply(function(x,y) x/y, x, scaling_factor_squared, SIMPLIFY = FALSE) )
## covariances done
# ----------------------------------------------------------------------------------- #
## put everything together according to the whole variance formula
# shift variances accordingly for ease of next computations (covariance already has the desired format)
variance_last <- append(variance, list(rep(list(rep(0, length(variance[[1]][[1]]))), X_cols))) # append zero element list
variance_first <- append(list(rep(list(rep(0, length(variance[[1]][[1]]))), X_cols)), variance) # prepend zero element list
# put everything together according to formula: var_last + var_first - cov
variance_me <- mapply(function(x,y,z) mapply(function(x,y,z) (x+y-z), x, y, z, SIMPLIFY = FALSE), variance_last, variance_first, covariance, SIMPLIFY = F)
# ----------------------------------------------------------------------------------- #
## output for final variances of marginal effects
# coerce to a matrix
variance_me <- sapply(variance_me, function(x) sapply(x, function(x) as.matrix(x)))
# add names
colnames(variance_me) <- sapply(categories, function(x) paste("Category", x, sep = " "))
rownames(variance_me) <- colnames(X)
# ----------------------------------------------------------------------------------- #
## standard deviations
# take square root of variance
sd_me <- sqrt(variance_me)
#### z scores and p values ####
t_value <- (marginal_effects)/(sd_me)
# control for dividing zero by zero
t_value[is.nan(t_value)] = 0
# p values
p_values <- 2*pnorm(-abs(t_value))
# ----------------------------------------------------------------------------------- #
} else {
# no values for the other parameters if inference is not desired
variance_me <- NULL
sd_me       <- NULL
t_value     <- NULL
p_values    <- NULL
# ----------------------------------------------------------------------------------- #
}
# ----------------------------------------------------------------------------------- #
# save forest information
forest_info <- list(inputs, categories, eval, window, newdata, inference)
names(forest_info) <- c("inputs", "categories", "eval", "window", "newData", "marginsInference")
# put everything into a list of results
results <- list(forest_info, marginal_effects, variance_me, sd_me, t_value, p_values)
names(results) <- c("forestInfo", "MarginalEffects", "Variances", "StandardErrors", "tValues", "pValues")
class(results) <- "margins.orf"
# return results
return(results)
# ----------------------------------------------------------------------------------- #
}
data(odata)
Y <- odata[, 1]
X <- odata[, -1]
set.seed(123)
orf <- orf(X, Y)
kktko <- margins(orf)
library(devtools)
document()
check()
library(orf)
# load data
data <- readRDS(file = "/home/okasag/Documents/HSG/facedata/Population_ALL_Subset.rds")
library(grf)
document()
document()
check()
library(orf)
document()
check()
library(orf)
#' repeat rows of a matrix
#'
#' function for replicating rows of a matrix n number of times
#'
#' @param matrix matrix which rows should be replicated
#' @param n number of times to repeat
#'
#'
rep_row<-function(matrix, n){
# thanks to: https://www.r-bloggers.com/a-quick-way-to-do-row-repeat-and-col-repeat-rep-row-rep-col/
matrix(rep(matrix, each = n), nrow = n)
}
document()
check()
library(orf)
document()
check()
library(orf)
document()
check()
library(orf)
library(devtools)
document()
check)
check()
library(orf)
sd_me <- 0.1
marginal_effects <- 0.5
#### z scores and p values ####
t_value <- (marginal_effects)/(sd_me)
t_value
# control for dividing zero by zero
t_value[is.nan(t_value)] = 0
t_value
# p values
p_values <- 2*pnorm(-abs(t_value))
p_values
document()
check()
library(orf)
# load example data
data(odata)
library(testthat)
# set X and Y for the estimation
Y <- odata[1:100, 1]
X <- odata[1:100, -1]
# testing the main orf function
test_that("orf object is of class orf", {
orf <- orf(X, Y)
expect_s3_class(orf, "orf")
})
# classes
test_that("predicted classes reflect actual classes", {
orf <- orf(X, Y)
orf_classes <- predict(orf, type = "class")$forestPredictions
expect_equal(sort(unique(as.numeric(orf_classes))), sort(unique(Y)))
})
orf <- orf(X, Y)
orf_classes <- predict(orf, type = "class")$forestPredictions
orf_classes
sort(unique(as.numeric(orf_classes)))
sort(unique(Y))
hist(as.numeric(orf_classes))
check()
document()
check()
library(orf)
# load example data
data(odata)
# set X and Y for the estimation
Y <- odata[1:100, 1]
X <- odata[1:100, -1]
set.seed(123)
orf <- orf(X, Y)
orf_classes <- predict(orf, type = "class")$forestPredictions
unique(as.numeric(orf_classes))
as.numeric(orf_classes)
hist(as.numeric(orf_classes))
set.seed(1)
orf <- orf(X, Y)
hist(as.numeric(orf_classes))
orf_classes <- predict(orf, type = "class")$forestPredictions
hist(as.numeric(orf_classes))
seq(1:nrow(odata))
sample(seq(1:nrow(odata)), 100)
# random sample of data
random <- sample(seq(1:nrow(odata)), 100)
# random sample of data
set.seed(123)
random <- sample(seq(1:nrow(odata)), 100)
# random sample of data
set.seed(123)
random <- sample(seq(1:nrow(odata)), 100)
# set X and Y for the estimation
Y <- odata[random, 1]
X <- odata[random, -1]
random
odata[random, 1]
odata[random, -1]
Y <- odata[1:100, 1]
# set X and Y for the estimation
Y <- odata[random, 1]
X <- odata[random, -1]
# testing the main orf function
test_that("orf object is of class orf", {
set.seed(123)
orf <- orf(X, Y)
expect_s3_class(orf, "orf")
})
test_that("orf object is of type list", {
set.seed(123)
orf <- orf(X, Y)
expect_type(orf, "list")
})
# forests
test_that("orf estimates ncat - 1 forests", {
set.seed(123)
orf <- orf(X, Y)
expect_equal(length(orf$trainForests), length(unique(Y))-1)
})
# probabilities
test_that("probabilities sum up to 1", {
set.seed(123)
orf <- orf(X, Y)
expect_equal(rowSums(orf$forestPredictions), rep(1, 100))
})
test_that("probabilities are non-negative", {
set.seed(123)
orf <- orf(X, Y)
expect_true(all(as.numeric(orf$forestPredictions) >= 0))
})
test_that("probabilities reflect all classes", {
set.seed(123)
orf <- orf(X, Y)
expect_equal(ncol(orf$forestPredictions), length(unique(Y)))
})
test_that("probabilities reflect all observations", {
set.seed(123)
orf <- orf(X, Y)
expect_equal(nrow(orf$forestPredictions), length(Y))
})
test_that("fitted values are equal to predicted values in training set", {
set.seed(123)
orf <- orf(X, Y)
orf_fitted <- orf$forestPredictions
orf_predicted <- predict(orf)$forestPredictions
expect_equal(orf_fitted, orf_predicted)
})
# classes
test_that("predicted classes reflect actual classes", {
set.seed(123)
orf <- orf(X, Y)
orf_classes <- predict(orf, type = "class")$forestPredictions
expect_equal(sort(unique(as.numeric(orf_classes))), sort(unique(Y)))
})
# variance
test_that("variances of the predictions are positive", {
set.seed(123)
orf <- orf(X, Y, inference = TRUE)
expect_true(all(as.numeric(orf$forestVariances) > 0))
})
set.seed(123)
orf <- orf(X, Y, inference = TRUE)
set.seed(123)
orf <- orf(X, Y, inference = TRUE)
set.seed(123)
orf <- orf(X, Y, inference = TRUE)
# load example data
data(odata)
# load example data
data(odata)
# set X and Y for the estimation
Y <- odata[1:200, 1]
X <- odata[1:200, -1]
# testing the main orf function
test_that("orf object is of class orf", {
set.seed(123)
orf <- orf(X, Y)
expect_s3_class(orf, "orf")
})
test_that("orf object is of type list", {
set.seed(123)
orf <- orf(X, Y)
expect_type(orf, "list")
})
# forests
test_that("orf estimates ncat - 1 forests", {
set.seed(123)
orf <- orf(X, Y)
expect_equal(length(orf$trainForests), length(unique(Y))-1)
})
# probabilities
test_that("probabilities sum up to 1", {
set.seed(123)
orf <- orf(X, Y)
expect_equal(rowSums(orf$forestPredictions), rep(1, 100))
})
test_that("probabilities are non-negative", {
set.seed(123)
orf <- orf(X, Y)
expect_true(all(as.numeric(orf$forestPredictions) >= 0))
})
test_that("probabilities reflect all classes", {
set.seed(123)
orf <- orf(X, Y)
expect_equal(ncol(orf$forestPredictions), length(unique(Y)))
})
test_that("probabilities reflect all observations", {
set.seed(123)
orf <- orf(X, Y)
expect_equal(nrow(orf$forestPredictions), length(Y))
})
test_that("fitted values are equal to predicted values in training set", {
set.seed(123)
orf <- orf(X, Y)
orf_fitted <- orf$forestPredictions
orf_predicted <- predict(orf)$forestPredictions
expect_equal(orf_fitted, orf_predicted)
})
# classes
test_that("predicted classes reflect actual classes", {
set.seed(123)
orf <- orf(X, Y)
orf_classes <- predict(orf, type = "class")$forestPredictions
expect_equal(sort(unique(as.numeric(orf_classes))), sort(unique(Y)))
})
# variance
test_that("variances of the predictions are positive", {
set.seed(123)
orf <- orf(X, Y, inference = TRUE)
expect_true(all(as.numeric(orf$forestVariances) > 0))
})
test_that("variances of the in sample predictions are the same for predict", {
set.seed(123)
orf <- orf(X, Y, inference = TRUE)
orf_vars <- orf$forestVariances
orf_preds <- predict(orf, inference = TRUE)$forestVariances
expect_equal(orf_vars, orf_preds)
})
test_that("variances without honesty and subsampling throw a warning", {
set.seed(123)
expect_message(orf(X, Y, honesty = FALSE, replace = TRUE, inference = TRUE))
set.seed(123)
expect_message(orf(X, Y, honesty = FALSE, replace = FALSE, inference = TRUE))
set.seed(123)
expect_message(orf(X, Y, honesty = TRUE, replace = TRUE, inference = TRUE))
})
# prediction error
test_that("prediction errors are positive", {
set.seed(123)
orf <- orf(X, Y)
expect_true(orf$forestAccuracy$MSE > 0)
expect_true(orf$forestAccuracy$RPS > 0)
})
# probabilities
test_that("probabilities sum up to 1", {
set.seed(123)
orf <- orf(X, Y)
expect_equal(rowSums(orf$forestPredictions), rep(1, nrow(X)))
})
set.seed(123)
orf <- orf(X, Y)
orf_classes <- predict(orf, type = "class")$forestPredictions
orf_classes
as.numeric(orf_classes)
hist(as.numeric(orf_classes))
document()
check()
document()
check()
library(orf)
orf
summary(orf)
orf$forestAccuracy
orf$trainForests
orf$forestImportance
orf$forestPredictions
document()
check()
library(orf)
orf_pred <- predict(orf)
orf_margins <- margins(orf)
document()
check()
library(orf)
orf$trainForests
orf <- orf(X,Y)
orf$trainForests
orf$forests
document()
check()
library(orf)
orf <- orf(X,Y)
orf
summary(orf)
orf$accuracy
orf$accuracy
orf$importance
orf$info
orf$accuracy
document()
check()
library(orf)
library(orf)
library(devtools)
document()
check()
document()
check()
library(orf)
document()
check()
library(orf)
document()
check()
library(orf)
citation(orf)
citation("orf")
citation("ranger")
citation("grf")
citation("party")
document()
check()
library(orf)
citation("orf")
144*31
usethis::use_citation()
licence()
RShowDoc("GPL-3")
update.packages(ask = FALSE)
install.packages("glmnet")
install.packages("ranger")
install.packages("devtools")
install.packages("knitr")
install.packages("rmarkdown")
