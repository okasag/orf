stop("Incorrect evaluation point. Programme terminated.")
}
# ----------------------------------------------------------------------------------- #
### get data needed for evaluation of ME
# get number of evaluation points
X_rows <- nrow(X_mean[[1]])
# get number of Xs
X_cols <- ncol(X_mean[[1]])
# get SD of Xs
X_sd <- rep_row(apply(X, 2, sd), n = X_rows)
# create X_up (X_mean + 0.1 * X_sd)
X_up <- X_mean[[1]] + h_std*X_sd
# create X_down (X_mean - 0.1 * X_sd)
X_down <- X_mean[[1]] - h_std*X_sd
## now check for the support of X
# check X_max
X_max <- rep_row(apply(X, 2, max), n = X_rows)
# check X_min
X_min <- rep_row(apply(X, 2, min), n = X_rows)
# check if X_up is within the range X_min and X_max
X_up <- (X_up < X_max) * X_up + (X_up >= X_max) * X_max
X_up <- (X_up > X_min) * X_up + (X_up <= X_min) * (X_min + h_std * X_sd)
# check if X_down is within the range X_min and X_max
X_down <- (X_down > X_min) * X_down + (X_down <= X_min) * X_min
X_down <- (X_down < X_max) * X_down + (X_down >= X_max) * (X_max - h_std * X_sd)
# check if X_up and X_down are same
if (any(X_up == X_down)) {
# adjust to higher share of SD
X_up   <- (X_up > X_down) * X_up   + (X_up == X_down) * (X_up   + 0.5 * h_std * X_sd)
X_down <- (X_up > X_down) * X_down + (X_up == X_down) * (X_down - 0.5 * h_std * X_sd)
# check the min max range again
X_up   <- (X_up < X_max) * X_up + (X_up >= X_max) * X_max
X_down <- (X_down > X_min) * X_down + (X_down <= X_min) * X_min
}
# checks for support of X done
# ----------------------------------------------------------------------------------- #
## now we need 2 datasets: one with X_up and second with X_down
# X_mean_up all (continous)
X_mean_up <- X_mean
X_mean_down <- X_mean
# replace values accordingly
for (i in 1:X_cols) {
X_mean_up[[i]][, i] <- X_up[, i]
X_mean_down[[i]][, i] <- X_down[, i]
}
# adjust for categorical X (works also for zero categorical)
for (i in X_categorical) {
X_mean_up[[i]][, i] <- ceiling(mean(X[, i]))
X_mean_down[[i]][, i] <- floor(mean(X[, i]))
}
# adjust for dummies (works also for zero dummies)
for (i in X_dummy) {
X_mean_up[[i]][, i] <- max(X[, i])
X_mean_down[[i]][, i] <- min(X[, i])
}
system.time(forest_weights_up <- predict_forest_weights_for_ME_fast(forest$trainForests, X, X_mean_up))
load("~/Documents/HSG/ORF/data/winequality_dataset_rdata.Rdata")
predict_forest_weights_for_ME_fast <- function(forest, data, pred_data) {
# get leafs only for number of forests and then check the Xs on their own
# prepare empty list
#forest_weights_up <- rep(list(rep(list(NA), ncol(data))), length(forest))
# ----------------------------------------------------------------------------------- #
# extract weights for desired Xs up
leaf_IDs <- lapply(forest, function(x) {
# first get terminal nodes, i.e get terminal nodes for each obs and each tree
# run your (new) data through the forest structure and look where your observations end up
leaf_IDs <- predict(x, data, type = "terminalNodes")$predictions
# put leaf_IDs into a list (one element for one tree)
lapply(seq_along(leaf_IDs[1, ]), function(i) leaf_IDs[, i])
})
# ----------------------------------------------------------------------------------- #
# get the leaf size as counts of observations in leaves
leaf_size <- lapply(leaf_IDs, function(x) {
lapply(x, function(x) ave(x, x, FUN = length))
})
# ----------------------------------------------------------------------------------- #
# start looping over all X_means
leaf_IDs_pred <- lapply(forest, function(x) {
# loop over Xs
lapply(seq_along(pred_data), function(X_index) {
# now do the same for your prediction data
leaf_IDs_pred <- predict(x, as.matrix(pred_data[[X_index]]), type = "terminalNodes")$predictions
# put leaf_IDs into a list
lapply(seq_along(leaf_IDs_pred[1, ]), function(i) leaf_IDs_pred[, i])
})
})
# ----------------------------------------------------------------------------------- #
## now compute the weights
# now average over the bootstraps, i.e. over trees to get final weights
# leaf_IDs_pred has one more level (Xs) than leaf_IDs and leaf_size
forest_weights_up <- lapply(seq_along(forest), function(forest_index) {
# go through forests and Xs
lapply(seq_along(pred_data), function(X_index) {
as(pred_weights_C(leaf_IDs_pred[[forest_index]][[X_index]], leaf_IDs[[forest_index]], leaf_size[[forest_index]]), "dgCMatrix")
})
})
# ----------------------------------------------------------------------------------- #
# return weights
return(forest_weights_up)
# ----------------------------------------------------------------------------------- #
}
# prepare data
dataset$y <- as.numeric(dataset$y)
X <- as.matrix(dataset[, 1:ncol(dataset)-1])
Y <- as.matrix(dataset[, ncol(dataset)])
# prepare parameters
ntree <- 10000
mtry <- ceiling(sqrt(ncol(X)))
nmin <- 1
honesty <- TRUE
inference <- FALSE
margins <- FALSE
ntree <- 10
# estimate orf in first place
set.seed(311992) # put your birthday here
# estimate model
orf_model <- orf(X,Y,ntree,mtry,nmin,honesty,inference,margins)
orf_model$forestInfo$inputs$inference <- TRUE
forest <- orf_model
eval = "mean"
newdata <- NULL
### decide if prediction or in sample marginal effects should be evaluated
if (is.null(newdata)) {
# if no newdata supplied, estimate in sample marginal effects
if (forest$forestInfo$inputs$honesty == FALSE) {
data <- forest$forestInfo$trainData # take in-sample data
} else if (forest$forestInfo$inputs$honesty == TRUE) {
data <- forest$forestInfo$honestData
}
} else {
# check if newdata is compatible with train data
if (ncol(newdata) != ncol(forest$forestInfo$trainData)) {
stop("newdata is not compatible with training data. Programme terminated.")
} else {
data = newdata
}
}
### data checks done
# ----------------------------------------------------------------------------------- #
### data preparation and checks
# get number of observations
n_data <- as.numeric(nrow(data))
# get categories
categories <- forest$forestInfo$categories
# get X as matrix
X <- as.matrix(data[, -1])
# get Y as matrix
Y <- as.matrix(data[, 1])
# create indicator variables (outcomes)
Y_ind <- lapply(categories[1:length(categories)-1], function(x) ifelse((Y <= x), 1, 0))
# create datasets with indicator outcomes
data_ind <- lapply(Y_ind, function(x) as.data.frame(cbind(as.matrix(unlist(x)), X)))
# ----------------------------------------------------------------------------------- #
### marginal effects preparation
# share of SD to be used
h_std <- 0.1
# check if X is continuous or dummy or categorical
X_type <- apply(X, 2, function(x) length(unique(x)))
# now determine the type of X
X_continuous <- which(X_type > 10) # define IDs of continuous Xs
X_dummy <- which(X_type == 2) # define IDs of dummies
X_categorical <- which(X_type > 2 & X_type <= 10)
# additional check for constant variables which are nonsensical
if (any(X_type == 1) | any(X_type == 0)) {
stop("Some of the covariates are constant. This is non-sensical for evaluation of marginal effects. Programme terminated.")
}
# ----------------------------------------------------------------------------------- #
### check the evaluation point
if (eval == "atmean") {
# variable of interest: X_1 to X_last, ME at mean
X_mean <- lapply(1:ncol(X), function(x) t(as.matrix(colMeans(X)))) # set all Xs to their mean values (so many times as we have Xs)
} else if (eval == "atmedian") {
# variable of interest: X_1 to X_last, ME at median
X_mean <- lapply(1:ncol(X), function(x) t(as.matrix(apply(X, 2, median)))) # set all Xs to their median values (so many times as we have Xs)
} else if (eval == "mean") {
# # variable of interest: X_1 to X_last, mean ME
X_mean <- lapply(1:ncol(X), function(x) X) # set all Xs to their exact values (so many times as we have Xs)
} else {
stop("Incorrect evaluation point. Programme terminated.")
}
# ----------------------------------------------------------------------------------- #
### get data needed for evaluation of ME
# get number of evaluation points
X_rows <- nrow(X_mean[[1]])
# get number of Xs
X_cols <- ncol(X_mean[[1]])
# get SD of Xs
X_sd <- rep_row(apply(X, 2, sd), n = X_rows)
# create X_up (X_mean + 0.1 * X_sd)
X_up <- X_mean[[1]] + h_std*X_sd
# create X_down (X_mean - 0.1 * X_sd)
X_down <- X_mean[[1]] - h_std*X_sd
## now check for the support of X
# check X_max
X_max <- rep_row(apply(X, 2, max), n = X_rows)
# check X_min
X_min <- rep_row(apply(X, 2, min), n = X_rows)
# check if X_up is within the range X_min and X_max
X_up <- (X_up < X_max) * X_up + (X_up >= X_max) * X_max
X_up <- (X_up > X_min) * X_up + (X_up <= X_min) * (X_min + h_std * X_sd)
# check if X_down is within the range X_min and X_max
X_down <- (X_down > X_min) * X_down + (X_down <= X_min) * X_min
X_down <- (X_down < X_max) * X_down + (X_down >= X_max) * (X_max - h_std * X_sd)
# check if X_up and X_down are same
if (any(X_up == X_down)) {
# adjust to higher share of SD
X_up   <- (X_up > X_down) * X_up   + (X_up == X_down) * (X_up   + 0.5 * h_std * X_sd)
X_down <- (X_up > X_down) * X_down + (X_up == X_down) * (X_down - 0.5 * h_std * X_sd)
# check the min max range again
X_up   <- (X_up < X_max) * X_up + (X_up >= X_max) * X_max
X_down <- (X_down > X_min) * X_down + (X_down <= X_min) * X_min
}
# checks for support of X done
# ----------------------------------------------------------------------------------- #
## now we need 2 datasets: one with X_up and second with X_down
# X_mean_up all (continous)
X_mean_up <- X_mean
X_mean_down <- X_mean
# replace values accordingly
for (i in 1:X_cols) {
X_mean_up[[i]][, i] <- X_up[, i]
X_mean_down[[i]][, i] <- X_down[, i]
}
# adjust for categorical X (works also for zero categorical)
for (i in X_categorical) {
X_mean_up[[i]][, i] <- ceiling(mean(X[, i]))
X_mean_down[[i]][, i] <- floor(mean(X[, i]))
}
# adjust for dummies (works also for zero dummies)
for (i in X_dummy) {
X_mean_up[[i]][, i] <- max(X[, i])
X_mean_down[[i]][, i] <- min(X[, i])
}
# ------------------
system.time(forest_weights_up <- predict_forest_weights_for_ME_fast(forest$trainForests, X, X_mean_up))
install.packages("bigmemory")
mem_used()
system.time(forest_pred_up <- predict_forest_preds_for_ME(forest$trainForests, data_ind, X_mean_up))
View(forest_pred_up)
rm(forest_weights_up)
gc()
system.time(forest_weights_up <- predict_forest_weights_for_ME(forest$trainForests, X, X_mean_up))
predict_forest_weights_for_ME_fast <- function(forest, data, pred_data) {
# get leafs only for number of forests and then check the Xs on their own
# prepare empty list
#forest_weights_up <- rep(list(rep(list(NA), ncol(data))), length(forest))
# ----------------------------------------------------------------------------------- #
# extract weights for desired Xs up
leaf_IDs <- lapply(forest, function(x) {
# first get terminal nodes, i.e get terminal nodes for each obs and each tree
# run your (new) data through the forest structure and look where your observations end up
leaf_IDs <- predict(x, data, type = "terminalNodes")$predictions
# put leaf_IDs into a list (one element for one tree)
lapply(seq_along(leaf_IDs[1, ]), function(i) leaf_IDs[, i])
})
# ----------------------------------------------------------------------------------- #
# get the leaf size as counts of observations in leaves
leaf_size <- lapply(leaf_IDs, function(x) {
lapply(x, function(x) ave(x, x, FUN = length))
})
# ----------------------------------------------------------------------------------- #
# start looping over all X_means
leaf_IDs_pred <- lapply(forest, function(x) {
# loop over Xs
lapply(seq_along(pred_data), function(X_index) {
# now do the same for your prediction data
leaf_IDs_pred <- predict(x, as.matrix(pred_data[[X_index]]), type = "terminalNodes")$predictions
# put leaf_IDs into a list
lapply(seq_along(leaf_IDs_pred[1, ]), function(i) leaf_IDs_pred[, i])
})
})
# ----------------------------------------------------------------------------------- #
## now compute the weights
# now average over the bootstraps, i.e. over trees to get final weights
# leaf_IDs_pred has one more level (Xs) than leaf_IDs and leaf_size
forest_weights_up <- lapply(seq_along(forest), function(forest_index) {
# go through forests and Xs
lapply(seq_along(pred_data), function(X_index) {
as(pred_weights_C(leaf_IDs_pred[[forest_index]][[X_index]], leaf_IDs[[forest_index]], leaf_size[[forest_index]]), "dgCMatrix")
})
})
# ----------------------------------------------------------------------------------- #
# return weights
return(forest_weights_up)
# ----------------------------------------------------------------------------------- #
}
devtools::load_all()
predict_forest_weights_for_ME_fast <- function(forest, data, pred_data) {
# get leafs only for number of forests and then check the Xs on their own
# prepare empty list
#forest_weights_up <- rep(list(rep(list(NA), ncol(data))), length(forest))
# ----------------------------------------------------------------------------------- #
# extract weights for desired Xs up
leaf_IDs <- lapply(forest, function(x) {
# first get terminal nodes, i.e get terminal nodes for each obs and each tree
# run your (new) data through the forest structure and look where your observations end up
leaf_IDs <- predict(x, data, type = "terminalNodes")$predictions
# put leaf_IDs into a list (one element for one tree)
lapply(seq_along(leaf_IDs[1, ]), function(i) leaf_IDs[, i])
})
# ----------------------------------------------------------------------------------- #
# get the leaf size as counts of observations in leaves
leaf_size <- lapply(leaf_IDs, function(x) {
lapply(x, function(x) ave(x, x, FUN = length))
})
# ----------------------------------------------------------------------------------- #
# start looping over all X_means
leaf_IDs_pred <- lapply(forest, function(x) {
# loop over Xs
lapply(seq_along(pred_data), function(X_index) {
# now do the same for your prediction data
leaf_IDs_pred <- predict(x, as.matrix(pred_data[[X_index]]), type = "terminalNodes")$predictions
# put leaf_IDs into a list
lapply(seq_along(leaf_IDs_pred[1, ]), function(i) leaf_IDs_pred[, i])
})
})
# ----------------------------------------------------------------------------------- #
## now compute the weights
# now average over the bootstraps, i.e. over trees to get final weights
# leaf_IDs_pred has one more level (Xs) than leaf_IDs and leaf_size
forest_weights_up <- lapply(seq_along(forest), function(forest_index) {
# go through forests and Xs
lapply(seq_along(pred_data), function(X_index) {
pred_weights_C(leaf_IDs_pred[[forest_index]][[X_index]], leaf_IDs[[forest_index]], leaf_size[[forest_index]])
})
})
# ----------------------------------------------------------------------------------- #
# return weights
return(forest_weights_up)
# ----------------------------------------------------------------------------------- #
}
load("~/Documents/HSG/ORF/data/winequality_dataset_rdata.Rdata")
# prepare data
dataset$y <- as.numeric(dataset$y)
X <- as.matrix(dataset[, 1:ncol(dataset)-1])
Y <- as.matrix(dataset[, ncol(dataset)])
# prepare parameters
ntree <- 10
mtry <- ceiling(sqrt(ncol(X)))
nmin <- 5
honesty <- TRUE
inference <- FALSE
margins <- FALSE
# estimate orf in first place
set.seed(311992) # put your birthday here
# estimate model
orf_model <- orf(X,Y,ntree,mtry,nmin,honesty,inference,margins)
orf_model$forestInfo$inputs$inference <- TRUE
forest <- orf_model
eval = "mean"
newdata <- NULL
### decide if prediction or in sample marginal effects should be evaluated
if (is.null(newdata)) {
# if no newdata supplied, estimate in sample marginal effects
if (forest$forestInfo$inputs$honesty == FALSE) {
data <- forest$forestInfo$trainData # take in-sample data
} else if (forest$forestInfo$inputs$honesty == TRUE) {
data <- forest$forestInfo$honestData
}
} else {
# check if newdata is compatible with train data
if (ncol(newdata) != ncol(forest$forestInfo$trainData)) {
stop("newdata is not compatible with training data. Programme terminated.")
} else {
data = newdata
}
}
### data checks done
# ----------------------------------------------------------------------------------- #
### data preparation and checks
# get number of observations
n_data <- as.numeric(nrow(data))
# get categories
categories <- forest$forestInfo$categories
# get X as matrix
X <- as.matrix(data[, -1])
# get Y as matrix
Y <- as.matrix(data[, 1])
# create indicator variables (outcomes)
Y_ind <- lapply(categories[1:length(categories)-1], function(x) ifelse((Y <= x), 1, 0))
# create datasets with indicator outcomes
data_ind <- lapply(Y_ind, function(x) as.data.frame(cbind(as.matrix(unlist(x)), X)))
# ----------------------------------------------------------------------------------- #
### marginal effects preparation
# share of SD to be used
h_std <- 0.1
# check if X is continuous or dummy or categorical
X_type <- apply(X, 2, function(x) length(unique(x)))
# now determine the type of X
X_continuous <- which(X_type > 10) # define IDs of continuous Xs
X_dummy <- which(X_type == 2) # define IDs of dummies
X_categorical <- which(X_type > 2 & X_type <= 10)
# additional check for constant variables which are nonsensical
if (any(X_type == 1) | any(X_type == 0)) {
stop("Some of the covariates are constant. This is non-sensical for evaluation of marginal effects. Programme terminated.")
}
# ----------------------------------------------------------------------------------- #
### check the evaluation point
if (eval == "atmean") {
# variable of interest: X_1 to X_last, ME at mean
X_mean <- lapply(1:ncol(X), function(x) t(as.matrix(colMeans(X)))) # set all Xs to their mean values (so many times as we have Xs)
} else if (eval == "atmedian") {
# variable of interest: X_1 to X_last, ME at median
X_mean <- lapply(1:ncol(X), function(x) t(as.matrix(apply(X, 2, median)))) # set all Xs to their median values (so many times as we have Xs)
} else if (eval == "mean") {
# # variable of interest: X_1 to X_last, mean ME
X_mean <- lapply(1:ncol(X), function(x) X) # set all Xs to their exact values (so many times as we have Xs)
} else {
stop("Incorrect evaluation point. Programme terminated.")
}
# ----------------------------------------------------------------------------------- #
### get data needed for evaluation of ME
# get number of evaluation points
X_rows <- nrow(X_mean[[1]])
# get number of Xs
X_cols <- ncol(X_mean[[1]])
# get SD of Xs
X_sd <- rep_row(apply(X, 2, sd), n = X_rows)
# create X_up (X_mean + 0.1 * X_sd)
X_up <- X_mean[[1]] + h_std*X_sd
# create X_down (X_mean - 0.1 * X_sd)
X_down <- X_mean[[1]] - h_std*X_sd
## now check for the support of X
# check X_max
X_max <- rep_row(apply(X, 2, max), n = X_rows)
# check X_min
X_min <- rep_row(apply(X, 2, min), n = X_rows)
# check if X_up is within the range X_min and X_max
X_up <- (X_up < X_max) * X_up + (X_up >= X_max) * X_max
X_up <- (X_up > X_min) * X_up + (X_up <= X_min) * (X_min + h_std * X_sd)
# check if X_down is within the range X_min and X_max
X_down <- (X_down > X_min) * X_down + (X_down <= X_min) * X_min
X_down <- (X_down < X_max) * X_down + (X_down >= X_max) * (X_max - h_std * X_sd)
# check if X_up and X_down are same
if (any(X_up == X_down)) {
# adjust to higher share of SD
X_up   <- (X_up > X_down) * X_up   + (X_up == X_down) * (X_up   + 0.5 * h_std * X_sd)
X_down <- (X_up > X_down) * X_down + (X_up == X_down) * (X_down - 0.5 * h_std * X_sd)
# check the min max range again
X_up   <- (X_up < X_max) * X_up + (X_up >= X_max) * X_max
X_down <- (X_down > X_min) * X_down + (X_down <= X_min) * X_min
}
# checks for support of X done
# ----------------------------------------------------------------------------------- #
## now we need 2 datasets: one with X_up and second with X_down
# X_mean_up all (continous)
X_mean_up <- X_mean
X_mean_down <- X_mean
# replace values accordingly
for (i in 1:X_cols) {
X_mean_up[[i]][, i] <- X_up[, i]
X_mean_down[[i]][, i] <- X_down[, i]
}
# adjust for categorical X (works also for zero categorical)
for (i in X_categorical) {
X_mean_up[[i]][, i] <- ceiling(mean(X[, i]))
X_mean_down[[i]][, i] <- floor(mean(X[, i]))
}
# adjust for dummies (works also for zero dummies)
for (i in X_dummy) {
X_mean_up[[i]][, i] <- max(X[, i])
X_mean_down[[i]][, i] <- min(X[, i])
}
# --------------------------
system.time(forest_weights_up <- predict_forest_weights_for_ME_fast(forest$trainForests, X, X_mean_up))
install.packages("bigmemory")
smallweights <- big.matrix(forest_weights_up[[1]][[1]])
library(bigmemory)
smallweights <- big.matrix(forest_weights_up[[1]][[1]])
smallweights <- big.matrix(forest_weights_up[[1]][[1]], ncol = ncol(forest_weights_up[[1]][[1]]))
forest_weights_up[[1]][[1]]
kktko <- forest_weights_up[[1]][[1]]
smallweights <- big.matrix(kktko, ncol = ncol(kktko))
ncol(kktko)
smallweights <- big.matrix(kktko, ncol = ncol(kktko), nrow = nrow(kktko))
smallweights <- as.big.matrix(kktko)
library(pryr)
object_size(kktko)
object_size(smallweights)
View(smallweights)
is.big.matrix(kktko)
is.big.matrix(smallweights)
smallweights
colMeans(smallweights)
read.big.matrix(smallweights)
mean(smallweights)
mean(smallweights[1:10,1])
mean(smallweights[1:10,2])
mean(smallweights[,2])
mean(smallweights[,26])
mean(smallweights[,2446])
mean(smallweights[1,2446])
mean(smallweights[1:2,2446])
mean(smallweights[1:10,2446])
mean(smallweights[1:20,2446])
mean(smallweights[, 1])
smallweights[, 1]
smallweights
smallweights[,]
colMeans(smallweights[,])
colMeans(kktko)
bigmeans <- colMeans(smallweights[,])
normalmeans <- colMeans(kktko)
all(bigmeans==normalmeans)
rm(forest_weights_up)
gc()
devtools::document()
devtools::check()
devtools::document()
devtools::check()
devtools::document()
devtools::check()
