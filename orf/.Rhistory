### get data needed for evaluation of ME
# get number of evaluation points
X_rows <- nrow(X_mean[[1]])
# get number of Xs
X_cols <- ncol(X_mean[[1]])
# get SD of Xs
X_sd <- rep_row(apply(X, 2, sd), n = X_rows)
# create X_up (X_mean + 0.1 * X_sd)
X_up <- X_mean[[1]] + h_std*X_sd
# create X_down (X_mean - 0.1 * X_sd)
X_down <- X_mean[[1]] - h_std*X_sd
## now check for the support of X
# check X_max
X_max <- rep_row(apply(X, 2, max), n = X_rows)
# check X_min
X_min <- rep_row(apply(X, 2, min), n = X_rows)
# check if X_up is within the range X_min and X_max
X_up <- (X_up < X_max) * X_up + (X_up >= X_max) * X_max
X_up <- (X_up > X_min) * X_up + (X_up <= X_min) * (X_min + h_std * X_sd)
# check if X_down is within the range X_min and X_max
X_down <- (X_down > X_min) * X_down + (X_down <= X_min) * X_min
X_down <- (X_down < X_max) * X_down + (X_down >= X_max) * (X_max - h_std * X_sd)
# check if X_up and X_down are same
if (any(X_up == X_down)) {
# adjust to higher share of SD
X_up   <- (X_up > X_down) * X_up   + (X_up == X_down) * (X_up   + 0.5 * h_std * X_sd)
X_down <- (X_up > X_down) * X_down + (X_up == X_down) * (X_down - 0.5 * h_std * X_sd)
# check the min max range again
X_up   <- (X_up < X_max) * X_up + (X_up >= X_max) * X_max
X_down <- (X_down > X_min) * X_down + (X_down <= X_min) * X_min
}
## now we need 2 datasets: one with X_up and second with X_down
# X_mean_up all (continous)
X_mean_up <- X_mean
X_mean_down <- X_mean
# replace values accordingly
for (i in 1:X_cols) {
X_mean_up[[i]][, i] <- X_up[, i]
X_mean_down[[i]][, i] <- X_down[, i]
}
X_mean_up
X_mean_down
i=6
X_mean_up[[i]][, i]
X_mean_down[[i]][, i]
ceiling(mean(X[, i]))
floor(mean(X[, i]))
ceiling(X_mean_up[[i]][, i])
floor(X_mean_up[[i]][, i])
i=15
X_mean_up[[i]][, i]
X_mean_down[[i]][, i]
max(X[, i])
min(X[, i])
X_mean_up[[i]][, i]
max(X_mean_up[[i]][, i])
ceiling(X_mean_up[[i]][, i])
ceiling(X_mean_up[[i]][, i])
floor(X_mean_down[[i]][, i])
ceiling(0.00001)
ceiling(0.9999999999999999999)
floor(0.9999999999999999999)
floor(0.9999999999999999999)
floor(0.99)
floor(0.9999)
# # variable of interest: X_1 to X_last, mean ME
X_mean <- lapply(1:ncol(X_eval), function(x) X_eval) # set all Xs to their exact values (so many times as we have Xs)
X_mean
### get data needed for evaluation of ME
# get number of evaluation points
X_rows <- nrow(X_mean[[1]])
# get number of Xs
X_cols <- ncol(X_mean[[1]])
# get SD of Xs
X_sd <- rep_row(apply(X, 2, sd), n = X_rows)
# create X_up (X_mean + 0.1 * X_sd)
X_up <- X_mean[[1]] + h_std*X_sd
# create X_down (X_mean - 0.1 * X_sd)
X_down <- X_mean[[1]] - h_std*X_sd
## now check for the support of X
# check X_max
X_max <- rep_row(apply(X, 2, max), n = X_rows)
# check X_min
X_min <- rep_row(apply(X, 2, min), n = X_rows)
# check if X_up is within the range X_min and X_max
X_up <- (X_up < X_max) * X_up + (X_up >= X_max) * X_max
X_up <- (X_up > X_min) * X_up + (X_up <= X_min) * (X_min + h_std * X_sd)
# check if X_down is within the range X_min and X_max
X_down <- (X_down > X_min) * X_down + (X_down <= X_min) * X_min
X_down <- (X_down < X_max) * X_down + (X_down >= X_max) * (X_max - h_std * X_sd)
# check if X_up and X_down are same
if (any(X_up == X_down)) {
# adjust to higher share of SD
X_up   <- (X_up > X_down) * X_up   + (X_up == X_down) * (X_up   + 0.5 * h_std * X_sd)
X_down <- (X_up > X_down) * X_down + (X_up == X_down) * (X_down - 0.5 * h_std * X_sd)
# check the min max range again
X_up   <- (X_up < X_max) * X_up + (X_up >= X_max) * X_max
X_down <- (X_down > X_min) * X_down + (X_down <= X_min) * X_min
}
## now we need 2 datasets: one with X_up and second with X_down
# X_mean_up all (continous)
X_mean_up <- X_mean
X_mean_down <- X_mean
# replace values accordingly
for (i in 1:X_cols) {
X_mean_up[[i]][, i] <- X_up[, i]
X_mean_down[[i]][, i] <- X_down[, i]
}
X_mean_up
X_mean
# variable of interest: X_1 to X_last, ME at mean
X_mean <- lapply(1:ncol(X_eval), function(x) t(as.matrix(colMeans(X_eval)))) # set all Xs to their mean values (so many times as we have Xs)
X_mean
# # variable of interest: X_1 to X_last, mean ME
X_mean <- lapply(1:ncol(X_eval), function(x) X_eval) # set all Xs to their exact values (so many times as we have Xs)
### get data needed for evaluation of ME
# get number of evaluation points
X_rows <- nrow(X_mean[[1]])
# get number of Xs
X_cols <- ncol(X_mean[[1]])
# get SD of Xs
X_sd <- rep_row(apply(X, 2, sd), n = X_rows)
# create X_up (X_mean + 0.1 * X_sd)
X_up <- X_mean[[1]] + h_std*X_sd
# create X_down (X_mean - 0.1 * X_sd)
X_down <- X_mean[[1]] - h_std*X_sd
## now check for the support of X
# check X_max
X_max <- rep_row(apply(X, 2, max), n = X_rows)
# check X_min
X_min <- rep_row(apply(X, 2, min), n = X_rows)
# check if X_up is within the range X_min and X_max
X_up <- (X_up < X_max) * X_up + (X_up >= X_max) * X_max
X_up <- (X_up > X_min) * X_up + (X_up <= X_min) * (X_min + h_std * X_sd)
# check if X_down is within the range X_min and X_max
X_down <- (X_down > X_min) * X_down + (X_down <= X_min) * X_min
X_down <- (X_down < X_max) * X_down + (X_down >= X_max) * (X_max - h_std * X_sd)
# check if X_up and X_down are same
if (any(X_up == X_down)) {
# adjust to higher share of SD
X_up   <- (X_up > X_down) * X_up   + (X_up == X_down) * (X_up   + 0.5 * h_std * X_sd)
X_down <- (X_up > X_down) * X_down + (X_up == X_down) * (X_down - 0.5 * h_std * X_sd)
# check the min max range again
X_up   <- (X_up < X_max) * X_up + (X_up >= X_max) * X_max
X_down <- (X_down > X_min) * X_down + (X_down <= X_min) * X_min
}
## now we need 2 datasets: one with X_up and second with X_down
# X_mean_up all (continous)
X_mean_up <- X_mean
X_mean_down <- X_mean
# replace values accordingly
for (i in 1:X_cols) {
X_mean_up[[i]][, i] <- X_up[, i]
X_mean_down[[i]][, i] <- X_down[, i]
}
X_mean_up
# adjust for categorical X (works also for zero categorical)
for (i in X_categorical) {
X_mean_up[[i]][, i] <- ceiling(X_mean_up[[i]][, i])
X_mean_down[[i]][, i] <- floor(X_mean_down[[i]][, i])
}
X_mean_up
X_mean_down
# adjust for dummies (works also for zero dummies)
for (i in X_dummy) {
X_mean_up[[i]][, i] <- max(X[, i])
X_mean_down[[i]][, i] <- min(X[, i])
}
X_mean_up[[i]][, i]
X_mean_up[[i]]
X_mean_up[[10]]
X_mean_down[[10]]
X_mean_up[[10]][1>10,]
X_mean_up[[10]][1:10,]
X_mean_down[[10]][1:10,]
# # variable of interest: X_1 to X_last, mean ME
X_mean <- lapply(1:ncol(X_eval), function(x) X_eval) # set all Xs to their exact values (so many times as we have Xs)
X_mean
### get data needed for evaluation of ME
# get number of evaluation points
X_rows <- nrow(X_mean[[1]])
# get number of Xs
X_cols <- ncol(X_mean[[1]])
# get SD of Xs
X_sd <- rep_row(apply(X, 2, sd), n = X_rows)
# create X_up (X_mean + 0.1 * X_sd)
X_up <- X_mean[[1]] + h_std*X_sd
# create X_down (X_mean - 0.1 * X_sd)
X_down <- X_mean[[1]] - h_std*X_sd
## now check for the support of X
# check X_max
X_max <- rep_row(apply(X, 2, max), n = X_rows)
# check X_min
X_min <- rep_row(apply(X, 2, min), n = X_rows)
# check if X_up is within the range X_min and X_max
X_up <- (X_up < X_max) * X_up + (X_up >= X_max) * X_max
X_up <- (X_up > X_min) * X_up + (X_up <= X_min) * (X_min + h_std * X_sd)
# check if X_down is within the range X_min and X_max
X_down <- (X_down > X_min) * X_down + (X_down <= X_min) * X_min
X_down <- (X_down < X_max) * X_down + (X_down >= X_max) * (X_max - h_std * X_sd)
# check if X_up and X_down are same
if (any(X_up == X_down)) {
# adjust to higher share of SD
X_up   <- (X_up > X_down) * X_up   + (X_up == X_down) * (X_up   + 0.5 * h_std * X_sd)
X_down <- (X_up > X_down) * X_down + (X_up == X_down) * (X_down - 0.5 * h_std * X_sd)
# check the min max range again
X_up   <- (X_up < X_max) * X_up + (X_up >= X_max) * X_max
X_down <- (X_down > X_min) * X_down + (X_down <= X_min) * X_min
}
## now we need 2 datasets: one with X_up and second with X_down
# X_mean_up all (continous)
X_mean_up <- X_mean
X_mean_down <- X_mean
X_mean_up
# replace values accordingly
for (i in 1:X_cols) {
X_mean_up[[i]][, i] <- X_up[, i]
X_mean_down[[i]][, i] <- X_down[, i]
}
X_mean_up[[i]]
X_mean_up[[i]]
X_mean_down[[i]]
# adjust for categorical X (works also for zero categorical)
for (i in X_categorical) {
X_mean_up[[i]][, i] <- ceiling(X_mean_up[[i]][, i])
X_mean_down[[i]][, i] <- floor(X_mean_down[[i]][, i])
}
# adjust for dummies (works also for zero dummies)
for (i in X_dummy) {
X_mean_up[[i]][, i] <- max(X[, i])
X_mean_down[[i]][, i] <- min(X[, i])
}
X_mean_up[[i]]
# # variable of interest: X_1 to X_last, mean ME
X_mean <- lapply(1:ncol(X_eval), function(x) X_eval) # set all Xs to their exact values (so many times as we have Xs)
### get data needed for evaluation of ME
# get number of evaluation points
X_rows <- nrow(X_mean[[1]])
# get number of Xs
X_cols <- ncol(X_mean[[1]])
# get SD of Xs
X_sd <- rep_row(apply(X, 2, sd), n = X_rows)
# create X_up (X_mean + 0.1 * X_sd)
X_up <- X_mean[[1]] + h_std*X_sd
# create X_down (X_mean - 0.1 * X_sd)
X_down <- X_mean[[1]] - h_std*X_sd
## now check for the support of X
# check X_max
X_max <- rep_row(apply(X, 2, max), n = X_rows)
# check X_min
X_min <- rep_row(apply(X, 2, min), n = X_rows)
# check if X_up is within the range X_min and X_max
X_up <- (X_up < X_max) * X_up + (X_up >= X_max) * X_max
X_up <- (X_up > X_min) * X_up + (X_up <= X_min) * (X_min + h_std * X_sd)
# check if X_down is within the range X_min and X_max
X_down <- (X_down > X_min) * X_down + (X_down <= X_min) * X_min
X_down <- (X_down < X_max) * X_down + (X_down >= X_max) * (X_max - h_std * X_sd)
# check if X_up and X_down are same
if (any(X_up == X_down)) {
# adjust to higher share of SD
X_up   <- (X_up > X_down) * X_up   + (X_up == X_down) * (X_up   + 0.5 * h_std * X_sd)
X_down <- (X_up > X_down) * X_down + (X_up == X_down) * (X_down - 0.5 * h_std * X_sd)
# check the min max range again
X_up   <- (X_up < X_max) * X_up + (X_up >= X_max) * X_max
X_down <- (X_down > X_min) * X_down + (X_down <= X_min) * X_min
}
## now we need 2 datasets: one with X_up and second with X_down
# X_mean_up all (continous)
X_mean_up <- X_mean
X_mean_down <- X_mean
# replace values accordingly
for (i in 1:X_cols) {
X_mean_up[[i]][, i] <- X_up[, i]
X_mean_down[[i]][, i] <- X_down[, i]
}
i = 10
X_mean_up[[i]]
X_mean_down[[i]]
X_mean[[i]]
X_up[, i]
ceiling(X_mean_up[[i]][, i])
floor(X_mean_down[[i]][, i])
mean(X[,i])
ceiling(X_mean_up[[i]][, i]) - floor(X_mean_down[[i]][, i])
sum(which((ceiling(X_mean_up[[i]][, i]) - floor(X_mean_down[[i]][, i])) == 2))
which((ceiling(X_mean_up[[i]][, i]) - floor(X_mean_down[[i]][, i])) == 2)
length(which((ceiling(X_mean_up[[i]][, i]) - floor(X_mean_down[[i]][, i])) == 2))
ceiling(X_mean_up[[i]][, i])
max(X[, i])
ceiling(X_mean_up[[i]][, i]) == max(X[, i])
ceiling(X_mean_up[[i]][, i])
ceiling(X_mean_down[[i]][, i])
ceiling(X_mean_up[[i]][, i]) - ceiling(X_mean_down[[i]][, i])
ceiling(X_mean_up[[i]][, i])
ifelse(ceiling(X_mean_down[[i]][, i]) == ceiling(X_mean_up[[i]][, i]),
floor(X_mean_up[[i]][, i]),
ceiling(X_mean_up[[i]][, i])
)
ceiling(X_mean_down[[i]][, i]) == ceiling(X_mean_up[[i]][, i])
ifelse(ceiling(X_mean_down[[i]][, i]) == ceiling(X_mean_up[[i]][, i]),
floor(X_mean_down[[i]][, i]),
ceiling(X_mean_down[[i]][, i])
)
ceiling(X_mean_up[[i]][, i])
ceiling(X_mean_up[[i]][, i]) - ifelse(ceiling(X_mean_down[[i]][, i]) == ceiling(X_mean_up[[i]][, i]),
floor(X_mean_down[[i]][, i]),
ceiling(X_mean_down[[i]][, i])
)
## now we need 2 datasets: one with X_up and second with X_down
# X_mean_up all (continous)
X_mean_up <- X_mean
X_mean_down <- X_mean
# replace values accordingly
for (i in 1:X_cols) {
X_mean_up[[i]][, i] <- X_up[, i]
X_mean_down[[i]][, i] <- X_down[, i]
}
# adjust for categorical X (works also for zero categorical) (adjustment such that the difference is always 1)
for (i in X_categorical) {
X_mean_up[[i]][, i] <- ceiling(X_mean_up[[i]][, i])
X_mean_down[[i]][, i] <- ifelse(ceiling(X_mean_down[[i]][, i]) == ceiling(X_mean_up[[i]][, i]),
floor(X_mean_down[[i]][, i]),
ceiling(X_mean_down[[i]][, i])
)
}
# adjust for dummies (works also for zero dummies)
for (i in X_dummy) {
X_mean_up[[i]][, i] <- max(X[, i])
X_mean_down[[i]][, i] <- min(X[, i])
}
X_mean_up[[i]][, i]
i = 10
X_mean_up[[i]][, i]
X_mean_down[[i]][, i]
document()
check()
library(orf)
X <- wine[, -1]
Y <- wine[, 1]
table(Y)
set.seed(1)
kktko <- orf(X,Y, num.trees = 1000, mtry = 3, min.node.size = 5, replace = FALSE, sample.fraction = 0.5, honesty = TRUE, inference = TRUE)
summary(kktko, latex = FALSE)
kktko_margins <- margins(kktko, eval = "atmean", window = NULL, newdata = NULL)
kktko_margins$MarginalEffects
kktko_margins
check_honesty_fraction <- function(honesty.fraction) {
if (is.null(honesty.fraction)) {
honesty.fraction <- 0.5
} else if (!is.numeric(honesty.fraction) | honesty.fraction <= 0 | honesty.fraction >= 1) {
stop("Error: Invalid value for honesty.fraction. honesty.fraction must be within [0,1] interval.")
}
honesty.fraction
}
honesty.fraction <- NULL
check_honesty_fraction(honesty.fraction)
honesty.fraction <- 1
check_honesty_fraction(honesty.fraction)
honesty.fraction <- 0
check_honesty_fraction(honesty.fraction)
honesty.fraction <- 0.1
check_honesty_fraction(honesty.fraction)
honesty.fraction <- 0.8
check_honesty_fraction(honesty.fraction)
# set working directory
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
## load dgp function
source("simulate_data_ORF_function.R")
# general simulation settings
n             <- 10000 # sample size
n_test        <- 1000 # test sample size
# number of discrete values for Y (3,6,9)
Y_n           <- 3
# settings for different DGPs (TRUE/FALSE) - use complicated GDP now
noise             <- TRUE # additional noise variables
highdim           <- FALSE # 1000 noise (zero signal) variables
nonlinear         <- TRUE # nonlinear effects as sin(2X)*beta
multi             <- TRUE # multicollinearity of X
randomcuts        <- FALSE # equal or random cutpoints for Y*
# simulate data
sim_data <- ologit_DGP(Y_n, n, n_test, multi, noise, highdim, nonlinear, randomcuts)
# remove everything
rm(list=setdiff(ls(), c("sim_data")))
sim_data
data <- sim_data$odata
data <- sim_data$odata[-1,]
honesty.fraction <- 0.5
#' check honesty fraction
#'
#' Checks the input data of honesty.fraction in orf
#'
#' @param honesty.fraction scalar, share of the data set aside to estimate the effects (default is 0.5)
#'
#' @return honesty.fraction
#'
check_honesty_fraction <- function(honesty.fraction) {
if (is.null(honesty.fraction)) {
honesty.fraction <- 0.5
} else if (!is.numeric(honesty.fraction) | honesty.fraction <= 0 | honesty.fraction >= 1) {
stop("Error: Invalid value for honesty.fraction. honesty.fraction must be within [0,1] interval.")
}
honesty.fraction
}
# get number of observations in total
n <- nrow(data)
# needed inputs for the function: data - dataframe which should be split into 50:50 sets
ind <- sample(c(rep(0, n/2), rep(1, n/2))) # randomize indicators
n/2
n/2
n/2
honesty.fraction
(1-honesty.fraction)
(1-honesty.fraction)*n
honesty.fraction*n
ceiling((1-honesty.fraction)*n))
ceiling((1-honesty.fraction)*n)
floor(honesty.fraction*n)
# get number of observations in total
n <- nrow(data)
# randomize indices for train and honest sample (take care of uneven numbers with floor and ceiling)
ind <- sample(c(rep(0, ceiling((1-honesty.fraction)*n)), rep(1, floor(honesty.fraction*n))))
# indicator for which observations go into train and honest set
honesty_i <- which(ind == 1)
# separate training set
train <- data[-honesty_i, ]
# separate honest set
honest <- data[honesty_i, ]
train
train[, 1]
unique(train[, 1])
all(sort(unique(train[, 1])) == sort(unique(honest[, 1])))
repeat{
# randomize indices for train and honest sample (take care of uneven numbers with floor and ceiling)
ind <- sample(c(rep(0, ceiling((1-honesty.fraction)*n)), rep(1, floor(honesty.fraction*n))))
# indicator for which observations go into train and honest set
honesty_i <- which(ind == 1)
# separate training set
train <- data[-honesty_i, ]
# separate honest set
honest <- data[honesty_i, ]
if(all(sort(unique(train[, 1])) == sort(unique(honest[, 1])))){
break
}
}
repeat{
# randomize indices for train and honest sample (take care of uneven numbers with floor and ceiling)
ind <- sample(c(rep(0, ceiling((1-honesty.fraction)*n)), rep(1, floor(honesty.fraction*n))))
# indicator for which observations go into train and honest set
honesty_i <- which(ind == 1)
# separate training set
train <- data[-honesty_i, ]
# separate honest set
honest <- data[honesty_i, ]
if(all(sort(unique(train[, 1])) == sort(unique(honest[, 1])))){
break
}
}
repeat{
# randomize indices for train and honest sample (take care of uneven numbers with floor and ceiling)
ind <- sample(c(rep(0, ceiling((1-honesty.fraction)*n)), rep(1, floor(honesty.fraction*n))))
# indicator for which observations go into train and honest set
honesty_i <- which(ind == 1)
# separate training set
train <- data[-honesty_i, ]
# separate honest set
honest <- data[honesty_i, ]
if(all(sort(unique(train[, 1])) == sort(unique(honest[, 1])))){
break
}
}
# initiate repeat indicator
rep_idx <- 1
# initiate repeat indicator
rep_idx <- 1
repeat{
# randomize indices for train and honest sample (take care of uneven numbers with floor and ceiling)
ind <- sample(c(rep(0, ceiling((1-honesty.fraction)*n)), rep(1, floor(honesty.fraction*n))))
# indicator for which observations go into train and honest set
honesty_i <- which(ind == 1)
# separate training set
train <- data[-honesty_i, ]
# separate honest set
honest <- data[honesty_i, ]
# check if in both data sets all outcome categories are represented or if too many tries have been done
if(all(sort(unique(train[, 1])) == sort(unique(honest[, 1]))) | rep_idx == 10){
break
}
# repeat indicator
rep_idx <- rep_idx + 1
}
(all(sort(unique(train[, 1])) != sort(unique(honest[, 1]))))
library(ranger)
ranger(Yo ~ .)
ranger(Yo ~ ., data = data)
ranger(as.numeric(Yo) ~ ., data = data)
boot1 <- ranger(as.numeric(Yo) ~ ., data = data)
set.seed(1)
boot1 <- ranger(as.numeric(Yo) ~ ., data = data, replace = TRUE, sample.fraction = 1)
summary(boot1)
boot1
set.seed(1)
boot05 <- ranger(as.numeric(Yo) ~ ., data = data, replace = TRUE, sample.fraction = 0.5)
boot05
trainData
colnames(data)
colnames(data)[-1]
inference = NULL
is.logical(inference)
X <- as.matrix(honest[,-1])
Y <- as.matrix(honest[,1])
Y <- as.matrix(as.numeric(honest[,1]))
orf(X,Y)
