forest <- forest$trainForests
## get train data names (only X)
train_data_name <- colnames(train_data)[2:ncol(train_data)]
if (is.null(newdata) & (inference == TRUE) & (inputs$inference == FALSE)) {
# note that the newdata is the estimation data and inference should reflect that
flag_newdata <- 1
# take traindata as newdata and estimate the weights needed for inference
newdata <- as.data.frame(rbind(train_data, honest_data))
# sort according to rownames
newdata <- as.data.frame(newdata[order(as.numeric(row.names(newdata))), ])
# get further values
n_newdata <- nrow(newdata) # rows of new data
n_cat <- as.numeric(length(categories))
} else if (!is.null(newdata)) {
## get X matrix newdata as dataframe and check colnames
# X
if (is.null(colnames(newdata))) { colnames(newdata) <- paste0("X", rep(1:ncol(newdata))) } # check if X has name
newdata_name <- colnames(newdata) # save the name of X
newdata <- as.data.frame(newdata) # as dataframe
n_newdata <- nrow(newdata) # rows of new data
n_cat <- as.numeric(length(categories))
# -------------------------------------------------------------------------------- #
# check if its compatible with the data used for training
if (all(train_data_name != newdata_name) | (ncol(newdata) != ncol(train_data)-1)) {
stop("Newdata are not compatible with the training data. Check supplied data. Programme terminated.")
}
}
# -------------------------------------------------------------------------------- #
## check inference possibilities according to previous estimation
# if inference TRUE, but orf was NOT estimated with subsampling AND honesty, no inference possible
if (inference == TRUE & (replace != FALSE | honesty != TRUE)) {
warning("Inference is not possible if the orf object was not estimated with both subsampling and honesty.
For predictions with inference, reestimate orf setting replace = FALSE and honesty = TRUE.")
inference <- FALSE
}
# -------------------------------------------------------------------------------- #
# check if honest forest was estimated and predict accordingly
if (honesty == FALSE & inference == FALSE) {
## no honest splitting, i.e. use all data, no inference
# predict with ncat-1 forests as in ranger default
pred <- lapply(forest, function(x) predict(x, data = newdata)$predictions)
# -------------------------------------------------------------------------------- #
# add the probability for the last outcome (always 1)
pred_1 <- append(pred, list(rep(1, n_newdata)))
# prepend zero vector to predictions for later differencing
pred_0 <- append(list(rep(0, n_newdata)), pred) # append a first 0 elemnt for the list
# --------------------------------------------------------------------------------------- #
# total predictions (make sure it returns a list)
pred_total <- as.list(mapply(function(x,y) x-y, pred_1, pred_0, SIMPLIFY = F))
# avoid negative predictions
pred_total <- lapply(pred_total, function(x) ifelse((x < 0), 0, x))
# coerce to final matrix
pred_total <- sapply(pred_total, function(x) as.matrix(x))
# normalize predictions
pred_final <- matrix(apply(pred_total, 1, function(x) (x)/(sum(x))), ncol = n_cat, byrow = T)
# add names
colnames(pred_final) <- sapply(categories, function(x) paste("Category", x, sep = " "))
# --------------------------------------------------------------------------------------- #
## convert probabilities into class predictions ("classification")
pred_class <- as.matrix(apply(pred_final, 1, which.max))
colnames(pred_class) <- "Category"
# --------------------------------------------------------------------------------------- #
# no variances
var_final <- NULL
# -------------------------------------------------------------------------------- #
} else if (honesty == TRUE & inference == FALSE) {
# -------------------------------------------------------------------------------- #
## run new Xs through estimated train forest and compute predictions based on honest sample
# no need to predict weights, get predictions directly through leaves
# predict with ncat-1 forests
pred <- mapply(function(x,y) predict_honest(x, y, newdata), forest, honest_ind_data, SIMPLIFY = FALSE)
# -------------------------------------------------------------------------------- #
# add the probability for the last outcome (always 1)
pred_1 <- append(pred, list(rep(1, n_newdata)))
# prepend zero vector to predictions for later differencing
pred_0 <- append(list(rep(0, n_newdata)), pred) # append a first 0 elemnt for the list
# --------------------------------------------------------------------------------------- #
# total predictions (make sure it returns a list)
pred_total <- as.list(mapply(function(x,y) x-y, pred_1, pred_0, SIMPLIFY = F))
# avoid negative predictions
pred_total <- lapply(pred_total, function(x) ifelse((x < 0), 0, x))
# coerce to final matrix
pred_total <- sapply(pred_total, function(x) as.matrix(x))
# normalize predictions
pred_final <- matrix(apply(pred_total, 1, function(x) (x)/(sum(x))), ncol = n_cat, byrow = T)
# add names
colnames(pred_final) <- sapply(categories, function(x) paste("Category", x, sep = " "))
# --------------------------------------------------------------------------------------- #
## convert probabilities into class predictions ("classification")
pred_class <- as.matrix(apply(pred_final, 1, which.max))
colnames(pred_class) <- "Category"
# --------------------------------------------------------------------------------------- #
# no variances
var_final <- NULL
# -------------------------------------------------------------------------------- #
} else if (honesty == TRUE & inference == TRUE) {
# -------------------------------------------------------------------------------- #
# predict weights by using forest train, honest data and newdata (for each category except one)
forest_weights_pred <- lapply(forest, function(x) predict_forest_weights(x, honest_data, newdata))
# get predictions by matrix multiplication of weights with honest responses
forest_pred <- mapply(function(x,y) as.numeric(x%*%y[, 1]), forest_weights_pred, honest_ind_data, SIMPLIFY = FALSE)
# -------------------------------------------------------------------------------- #
# add the probability for the last outcome (always 1)
pred_1 <- append(forest_pred, list(rep(1, n_newdata)))
# prepend zero vector to predictions for later differencing
pred_0 <- append(list(rep(0, n_newdata)), forest_pred) # append a first 0 elemnt for the list
# --------------------------------------------------------------------------------------- #
# total predictions (make sure it returns a list)
pred_total <- as.list(mapply(function(x,y) x-y, pred_1, pred_0, SIMPLIFY = F))
# avoid negative predictions
pred_total <- lapply(pred_total, function(x) ifelse((x < 0), 0, x))
# coerce to final matrix
pred_total <- sapply(pred_total, function(x) as.matrix(x))
# normalize predictions
pred_final <- matrix(apply(pred_total, 1, function(x) (x)/(sum(x))), ncol = n_cat, byrow = T)
# add names
colnames(pred_final) <- sapply(categories, function(x) paste("Category", x, sep = " "))
# --------------------------------------------------------------------------------------- #
## convert probabilities into class predictions ("classification")
pred_class <- as.matrix(apply(pred_final, 1, which.max))
colnames(pred_class) <- "Category"
# --------------------------------------------------------------------------------------- #
# adapt variance computations according to newdata
if (flag_newdata == 1) {
# use get_orf_variance
# compute variances as in within estimation (smaller normalization parameter due to sample size)
# divide forest_pred and forest_weights into 2 pieces for honest and train (ordering doesnt matter)
n_halfdata <- floor(length(forest_pred[[1]])/2)
n_fulldata <- length(forest_pred[[1]])
# transform to matrix vector
forest_pred <- lapply(forest_pred, function(x) matrix(x, ncol = 1))
# take care of rownames
forest_pred <- lapply(forest_pred, function(x) { rownames(x) <- (1:n_fulldata); return(x) })
forest_weights_pred <- lapply(forest_weights_pred, function(x) { rownames(x) <- (1:n_fulldata); return(x) })
# predictions (vectors as matrices)
honest_pred <- lapply(forest_pred, function(x) as.matrix(x[(1:n_halfdata), ]))
train_pred  <- lapply(forest_pred, function(x) as.matrix(x[((n_halfdata + 1):n_fulldata), ]))
# weights (matrices)
honest_weights_pred <- lapply(forest_weights_pred, function(x) x[(1:n_halfdata), ])
train_weights_pred  <- lapply(forest_weights_pred, function(x) x[((n_halfdata + 1):n_fulldata), ])
# get indicator outcomes out of honest indicator data
Y_ind_honest <- lapply(honest_ind_data, function(x) matrix(x[, 1], ncol = 1))
# compute the in sample variance
var_final <- get_orf_variance(honest_pred, honest_weights_pred, train_pred, train_weights_pred, Y_ind_honest)
} else {
# use standard pred_orf_variance
# get indicator outcomes out of honest indicator data
Y_ind_honest <- lapply(honest_ind_data, function(x) x[, 1])
# compute the variances for the categorical predictions
var_final <- pred_orf_variance(forest_pred, forest_weights_pred, Y_ind_honest)
}
# -------------------------------------------------------------------------------- #
}
# -------------------------------------------------------------------------------- #
}
# -------------------------------------------------------------------------------- #
# save forest information
forest_info <- list(inputs, categories, newdata)
names(forest_info) <- c("inputs", "categories", "newData")
# define output of the function
output <- list(forest_info, pred_final, var_final, pred_class)
names(output) <- c("forestInfo", "forestPredictions", "forestVariances", "predictedCategories")
# -------------------------------------------------------------------------------- #
## Set the name for the class
class(output) <- "orf.prediction"
# return output
return(output)
# -------------------------------------------------------------------------------- #
}
#' plot.orf
#'
#' plot ordered random forest object of class \code{orf}
#'
#' @param x estimated ordered random forest object of type \code{orf}
#' @param ... further arguments (currently ignored)
#'
#' @import ggplot2
#' @importFrom utils stack
#'
#' @export
plot.orf <- function(x, ...) {
# needed inputs for the function: forest - forest object coming from orf function
# -------------------------------------------------------------------------------- #
## get forest as x
forest <- x
## save forest inputs
inputs <- forest$forestInfo$inputs
honesty <- inputs$honesty
categories <- forest$forestInfo$categories
honest_data <- forest$forestInfo$honestData
train_data <- forest$forestInfo$trainData
# -------------------------------------------------------------------------------- #
# get predictions and estimation data
probabilities <- forest$forestPredictions # take out honest predictions
all_data <- rbind(honest_data, train_data) # put data together
all_data <- all_data[order(as.numeric(row.names(all_data))), ] # sort data as original
outcomes <- all_data[, 1] # take the observed outcomes
# -------------------------------------------------------------------------------- #
### plot ORF ###
## plot realized categories overlayed with predicted category probabilities
# new colnames
colnames(probabilities) <- sapply(seq_along(categories), function(i) paste0("P(Y=", i, ")"))
# cbind together
df_plot <- as.data.frame(cbind(outcomes, probabilities))
# subset according to categories
df_plot_cat <- lapply(seq_along(categories), function(i) as.data.frame(subset(df_plot, outcomes == i)))
# take colmeans
df_cat_means <- lapply(df_plot_cat, function(x) t(as.matrix(colMeans(x)[seq_along(categories)+1])))
# add colmeans to df_plot_cat
df_plot_cat <- mapply(function(x,y) cbind(x, y), df_plot_cat, df_cat_means, SIMPLIFY = FALSE)
# reshape data for ggplot
df_plot_prob <- lapply(df_plot_cat, function(x) stack(x[seq_along(categories)+1]))
df_plot_mean <- lapply(df_plot_cat, function(x) stack(x[seq_along(categories)+1+length(categories)]))
# add colnames and column indicating the outcome category
df_plot_prob <- lapply(seq_along(df_plot_prob), function(i) {
# add column of outcome category to eahc list entry
df_plot_prob[[i]]$Outcome <- paste("Class", i, sep = " ")
# add colnames
colnames(df_plot_prob[[i]]) <- c("Probability", "Density", "Outcome")
# return the list
return(df_plot_prob[[i]])  })
# stack the dataframes under each other
df_plot_prob <- as.data.frame(do.call(rbind, df_plot_prob))
# add colnames and column indicating the outcome category
df_plot_mean <- lapply(seq_along(df_plot_mean), function(i) {
# add column of outcome category to eahc list entry
df_plot_mean[[i]]$Outcome <- paste("Class", i, sep = " ")
# add colnames
colnames(df_plot_mean[[i]]) <- c("Probability", "Density", "Outcome")
# return the list
return(df_plot_mean[[i]])  })
# stack the dataframes under each other
df_plot_mean <- as.data.frame(do.call(rbind, df_plot_mean))
# -------------------------------------------------------------------------------- #
# generate ggplot
ggplot(data = df_plot_prob, aes_string(x = "Probability", fill = "Density"))+
geom_density(alpha = 0.4, aes_string(y = "..scaled..")) +
facet_wrap("Outcome", ncol = 1)+
geom_vline(data = df_plot_mean, aes_string(xintercept = "Probability", color = "Density"), linetype="dashed") +
ggtitle("Distribution of Ordered Forest Probability Predictions") +
xlab("Predicted Probability") +
ylab("Probability Mass") +
theme_bw() +
theme(strip.background = element_rect(fill = "gray92")) +
theme(legend.position = "top") +
theme(plot.title = element_text(hjust = 0.5))
# no output to return for plot
# -------------------------------------------------------------------------------- #
}
#' summary.orf
#'
#' summary of an ordered random forest object of class \code{orf}
#'
#' @param object estimated ordered random forest object of type \code{orf}
#' @param latex logical, TRUE if latex summary should be generated
#' @param ... further arguments (currently ignored)
#'
#' @importFrom xtable xtable
#'
#' @export
summary.orf <- function(object, latex = FALSE, ...) {
# needed inputs for the function: forest - forest object coming from random_forest function
#                                        - latex : logical if the output should be printed in latex code
# -------------------------------------------------------------------------------- #
## check user inputs
latex <- check_latex(latex)
## get forest as object
forest <- object
# -------------------------------------------------------------------------------- #
## save forest inputs
main_class        <- class(forest)[1]
inputs            <- forest$forestInfo$inputs
honesty           <- inputs$honesty
honesty.fraction  <- inputs$honesty.fraction
inference         <- inputs$inference
importance        <- inputs$importance
mtry              <- inputs$mtry
num.trees         <- inputs$num.trees
min.node.size     <- inputs$min.node.size
replace           <- inputs$replace
sample.fraction   <- inputs$sample.fraction
honest_data       <- forest$forestInfo$honestData
train_data        <- forest$forestInfo$trainData
categories        <- length(forest$forestInfo$categories)
type              <- "Ordered Forest"
# -------------------------------------------------------------------------------- #
## honest splitting, i.e. use honest data
# take out summary statistics
mse         <- round(forest$MSE, 5)
rps         <- round(forest$RPS, 5)
trainsize   <- nrow(train_data)
honestsize  <- ifelse(is.null(honest_data), 0, nrow(honest_data))
features    <- ncol(train_data) - 1   # take out the response
# check if subsampling or bootstrapping was used
if (forest$trainForests[[1]]$replace == TRUE) { build <- "Bootstrap" } else { build <- "Subsampling" }
# -------------------------------------------------------------------------------- #
# structure summary into a list
output        <- list(type, categories, build, num.trees, mtry, min.node.size, replace, sample.fraction, honesty, honesty.fraction, inference, importance, trainsize, honestsize, features, mse, rps)
names(output) <- c("type", "categories", "build", "num.trees", "mtry", "min.node.size", "replace", "sample.fraction", "honesty", "honesty.fraction", "inference", "importance", "trainsize", "honestsize", "features", "mse", "rps")
# output matrix
output_matrix <- matrix(NA, ncol = 1, nrow = length(output))
# populate output matrix
rownames(output_matrix) <- names(output) # rownames are names
colnames(output_matrix) <- "" # no visible colname
output_matrix[, 1]      <- unlist(output) # column 1 are values
# generate latex output if selected
if (latex == TRUE) { colnames(output_matrix) <- "Ordered Forest Summary"
output_matrix <- xtable(output_matrix, caption = "Summary of the Ordered Forest Estimation", align = "ll")
}
# pack it into output
output <- output_matrix
# -------------------------------------------------------------------------------- #
cat("Summary of the", type, "Estimation \n\n")
# return output
print(noquote(output), comment = FALSE)
# -------------------------------------------------------------------------------- #
}
#' print.orf
#'
#' print of an ordered forest object of class \code{orf}
#'
#' @param x object of type \code{margins.orf}
#' @param ... further arguments (currently ignored)
#'
#' @export
print.orf <- function(x, ...) {
# needed inputs for the function: forest - forest object coming from orf function
#                                        - ... additional arguments (currently ignored)
# -------------------------------------------------------------------------------- #
## get forest as x
forest <- x
# -------------------------------------------------------------------------------- #
## save forest inputs
main_class        <- class(forest)[1]
inputs            <- forest$forestInfo$inputs
honesty           <- inputs$honesty
mtry              <- inputs$mtry
num.trees         <- inputs$num.trees
min.node.size     <- inputs$min.node.size
replace           <- inputs$replace
inference         <- inputs$inference
honest_data       <- forest$forestInfo$honestData
train_data        <- forest$forestInfo$trainData
categories        <- length(forest$forestInfo$categories)
build             <- ifelse(replace == TRUE, "Bootstrap", "Subsampling")
type              <- "Ordered Forest"
# -------------------------------------------------------------------------------- #
cat(type, "object of class", main_class, "\n\n")
cat("Number of Categories:            ", categories, "\n")
cat("Sample Size:                     ", sum(nrow(train_data), nrow(honest_data)), "\n")
cat("Number of Trees:                 ", num.trees, "\n")
cat("Build:                           ", build, "\n")
cat("Mtry:                            ", mtry, "\n")
cat("Minimum Node Size:               ", min.node.size, "\n")
cat("Honest Forest:                   ", honesty, "\n")
cat("Weight-Based Inference:          ", inference, "\n")
# -------------------------------------------------------------------------------- #
}
var_imp
matrix(unlist(lapply(Y_ind_train, function(x) mean(x)), nrow = 1)
)
unlist(lapply(Y_ind_train, function(x) mean(x))
)
matrix(unlist(lapply(Y_ind_train, function(x) mean(x))), nrow = 1)
var_imp <- matrix(unlist(lapply(Y_ind_train, function(x) mean(x))), nrow = 1)
var_imp
apply(var_imp, 1, function(x) (x)/(sum(x)))
matrix(apply(var_imp, 1, function(x) (x)/(sum(x))), nrow = 1)
sum(matrix(apply(var_imp, 1, function(x) (x)/(sum(x))), nrow = 1))
0.258 + 0.504 + 0.734
lapply(Y_ind_train, function(x) mean(x))
(0.258/0.734) + ((0.504-0.258)/0.734) + ((0.734-0.504)/0.734)
(0.258/0.734)
((0.504-0.258)/0.734)
((0.734-0.504)/0.734)
var_imp
lapply(Y_ind_train, function(x) mean(x)))
lapply(Y_ind_train, function(x) mean(x))
var_imp <- lapply(Y_ind_train, function(x) mean(x))
var_imp
var_imp <- matrix(unlist(lapply(Y_ind_train, function(x) mean(x))), nrow = 1)
var_imp
seq_along(var_imp)
lapply(seq_along(var_imp), function(i) print(i))
var_imp_0 <- cbind(0, var_imp)
var_imp_0 <- cbind(0, var_imp[1:(ncol(var_imp)-1)])
var_imp_0
var_imp[1:(ncol(var_imp)-1)]
var_imp
var_imp[1:(ncol(var_imp)-1)]
var_imp_0 <- cbind(0, var_imp[1, 1:(ncol(var_imp)-1)])
var_imp
var_imp_0
var_imp_0 <- c(0, var_imp[1, 1:(ncol(var_imp)-1)])
var_imp_0
var_imp <- matrix(unlist(lapply(Y_ind_train, function(x) mean(x))), nrow = 1)
var_imp_0 <- matrix(c(0, var_imp[1, 1:(ncol(var_imp)-1)]), nrow = 1)
var_imp
var_imp_0
var_imp
var_imp_last <- var_imp[1, ncol(var_imp)]
var_imp_last
# pre-requesities for variable importance computation
var_imp_shares      <- matrix(unlist(lapply(Y_ind_train, function(x) mean(x))), nrow = 1) # matrix of proportions for each but last class
var_imp_shares_0    <- matrix(c(0, var_imp[1, 1:(ncol(var_imp)-1)]), nrow = 1) # shifted matrix for ease of computation
var_imp_shares_last <- var_imp[1, ncol(var_imp)]
forest
forest[[1]]
forest[[1]]$variable.importance
lapply(forest, function(x) x$variable.importance)
unlist(lapply(forest, function(x) x$variable.importance))
var_imp_shares
# pre-requesities for variable importance computation
var_imp_shares      <- matrix(unlist(lapply(Y_ind_train, function(x) mean(x))), nrow = 1) # matrix of proportions for each but last class
var_imp_shares_0    <- matrix(c(0, var_imp_shares[1, 1:(ncol(var_imp_shares)-1)]), nrow = 1) # shifted matrix for ease of computation
var_imp_shares_last <- var_imp_shares[1, ncol(var_imp_shares)] # get the last share of classes
var_imp_forests     <- matrix(unlist(lapply(forest, function(x) x$variable.importance)), ncol = ncol(var_imp_shares)) # get the variable importances for eahc of binary forests
var_imp_forests
var_imp_forests     <- matrix(unlist(lapply(forest, function(x) x$variable.importance)), nrow = ncol(var_imp_shares)) # get the variable importances for eahc of binary forests
var_imp_forests
lapply(forest, function(x) x$variable.importance)
unlist(lapply(forest, function(x) x$variable.importance))
var_imp_forests     <- matrix(unlist(lapply(forest, function(x) x$variable.importance)), nrow = ncol(var_imp_shares), byrow = T) # get the variable importances for eahc of binary forests
var_imp_forests
var_imp_shares
var_imp_shares - var_imp_shares_0
var_imp_shares_0
(var_imp_shares - var_imp_shares_0)/var_imp_shares_last
t((var_imp_shares - var_imp_shares_0)/var_imp_shares_last)
(t((var_imp_shares - var_imp_shares_0)/var_imp_shares_last))
# compute var_imp using shares and importance form the binary forests
var_imp <- (t((var_imp_shares - var_imp_shares_0)/var_imp_shares_last)) * var_imp_forests
var_imp_forests
(t((var_imp_shares - var_imp_shares_0)/var_imp_shares_last))
(t((var_imp_shares - var_imp_shares_0)/var_imp_shares_last)) * var_imp_forests
# compute scaling factor using shares
var_imp_scaling     <- (t((var_imp_shares - var_imp_shares_0)/var_imp_shares_last))
var_imp_scaling
var_imp_forests
(t((var_imp_shares - var_imp_shares_0)/var_imp_shares_last))
ncol(X_train)
rep((t((var_imp_shares - var_imp_shares_0)/var_imp_shares_last)), ncol(X_train))
matrix((t((var_imp_shares - var_imp_shares_0)/var_imp_shares_last)), ncol = ncol(X_train))
t((var_imp_shares - var_imp_shares_0)/var_imp_shares_last))
t((var_imp_shares - var_imp_shares_0)/var_imp_shares_last)
matrix(t((var_imp_shares - var_imp_shares_0)/var_imp_shares_last), ncol = 4)
matrix(rep(t((var_imp_shares - var_imp_shares_0)/var_imp_shares_last), 4) ncol = 4)
matrix(rep(t((var_imp_shares - var_imp_shares_0)/var_imp_shares_last), 4), ncol = 4)
# compute scaling factor using shares
var_imp_scaling     <- matrix(rep(t((var_imp_shares - var_imp_shares_0)/var_imp_shares_last), ncol(X_train)), ncol = ncol(X_train))
var_imp_scaling
var_imp_scaling * var_imp_forests
colSums(var_imp_scaling * var_imp_forests)
# compute var_imp using scaling factor and importance from binary forests
var_imp <- colSums(var_imp_scaling * var_imp_forests)
var_imp
var_imp_forests
# compute var_imp using scaling factor and importance from binary forests
var_imp <- round(colSums(var_imp_scaling * var_imp_forests), 4)
var_imp
rowSums(var_imp)
sum(var_imp)
# compute var_imp using scaling factor and importance from binary forests
var_imp <- as.numeric(round(colSums(var_imp_scaling * var_imp_forests), 4))
var_imp
names(var_imp) <- X_name
var_imp
forest[[1]]$variable.importance
document()
check()
library(orf)
X <- as.matrix(dfOrd[, 1:4])
Y <- as.matrix(as.numeric(dfOrd[, 5]))
library(orf)
orf <- orf(X,Y)
plot(orf)
library(orf)
orf <- orf(X,Y)
plot(orf)
set.seed(1)
N     <- 1000
X1    <- rnorm(N, 100, 10)
X2    <- abs(runif(N, 0, 10))
X3    <- rbinom(N, 1, 0.5)
X4    <- rnorm(N, 0, 1)
Ycont <- 1*(X1) + 2*(X2) + 3*(X3) + rnorm(N, 0, 1)
Yord  <- cut(Ycont, breaks=quantile(Ycont), include.lowest=TRUE,
labels=c("1", "2", "3", "4"), ordered=TRUE)
dfOrd <- data.frame(X1, X2, X3, X4, Yord)
X <- as.matrix(dfOrd[, 1:4])
Y <- as.matrix(as.numeric(dfOrd[, 5]))
library(orf)
orf <- orf(X,Y)
plot(orf)
orf
summary(orf)
orf <- orf(X,Y, importance = TRUE)
plot(orf)
orf
summary(orf)
orf$variableImportance
orf <- orf(X,Y, importance = kkt)
orf <- orf(X,Y, importance = 1)
orf$variableImportance
library(devtools)
document
document()
check()
library(orf)
vignette(orf)
vignette("orf")
vignette(package = "orf")
