X_mean <- lapply(1:ncol(X), function(x) colMeans(X)) # set all Xs to their mean values (so many times as we have Xs)
} else if (eval=="median") {
# variable of interest: X_1 to X_last, ME at median
X_mean <- lapply(1:ncol(X), function(x) apply(X, 2, median)) # set all Xs to their median values (so many times as we have Xs)
} else {
stop("Incorrect evaluation point. Programme terminated.")
}
# get SD of Xs
X_sd <- apply(X, 2, sd)
# create X_up (X_mean + 0.1 * X_sd)
X_up <- X_mean[[1]] + h_std*X_sd
# create X_down (X_mean - 0.1 * X_sd)
X_down <- X_mean[[1]] - h_std*X_sd
## now check for the support of X
# check X_max
X_max <- apply(X, 2, max)
# check X_min
X_min <- apply(X, 2, min)
# check if X_up is within the range X_min and X_max
X_up <- (X_up < X_max) * X_up + (X_up >= X_max) * X_max
X_up <- (X_up > X_min) * X_up + (X_up <= X_min) * (X_min + h_std * X_sd)
# check if X_down is within the range X_min and X_max
X_down <- (X_down > X_min) * X_down + (X_down <= X_min) * X_min
X_down <- (X_down < X_max) * X_down + (X_down >= X_max) * (X_max - h_std * X_sd)
# some additional checks
## now we need 2 datasets: one with X_up and second with X_down
# X_mean_up
X_mean_up <- lapply(seq_along(X_mean), function(i) replace(X_mean[[i]], i, X_up[i]) )
# X_mean_down
X_mean_down <- lapply(seq_along(X_mean), function(i) replace(X_mean[[i]], i, X_down[i]) )
# adjust for categorical X (works also for zero categorical)
for (i in X_categorical) {
X_mean_up[[i]][[i]] <- ceiling(mean(X[,i]))
X_mean_down[[i]][[i]] <- floor(mean(X[,i]))
}
# adjust for dummies (works also for zero dummies)
for (i in X_dummy) {
X_mean_up[[i]][[i]] <- max(X[,i])
X_mean_down[[i]][[i]] <- min(X[,i])
}
################################################################
# essentials for computations of marginal effects are prepared #
################################################################
# extract weights for desired Xs up: get weights from honest sample and predict weights for evaluation points from HONEST sample
forest_weights_up <- predict_forest_weights_for_ME(forest, X, X_mean_up)
# extract weights for desired Xs down
forest_weights_down <- predict_forest_weights_for_ME(forest, X, X_mean_down)
# now subtract the weights according to the ME formula
forest_weights_diff_up_down <- mapply(function(x,y) mapply(function(x,y) x-y, x, y,  SIMPLIFY = F), forest_weights_up, forest_weights_down, SIMPLIFY = F)
## compute prerequisities for marginal effects
# compute the conditional means: weights%*%y (predictions are based on honest sample)
forest_cond_means <- mapply(function(x,y) lapply(x, function(x) x%*%y), forest_weights_diff_up_down, Y_ind_honest, SIMPLIFY = FALSE)
# subtract conditional means according to formula to isolate categories
forest_cond_means_0_last <- append(forest_cond_means, list(rep(list(0), ncol(X)))) # append zero elemnt list
forest_cond_means_0_first <- append(list(rep(list(0), ncol(X))), forest_cond_means) # prepend zero element list
# compute the scaling factor: X_up-X_down=2*X_sd
scaling_factor <- as.list(X_up - X_down)
# set scaling factor to zero for categorical and dummy variables
for (i in X_categorical & X_dummy) {
scaling_factor[[i]] <- 0
}
# now compute the differences for marginal effects
marginal_effects_diff <- mapply(function(x,y) mapply(function(x,y) x-y, x, y, SIMPLIFY = F), forest_cond_means_0_last, forest_cond_means_0_first, SIMPLIFY = F)
# scale marginal effects
marginal_effects_scaled <- lapply(marginal_effects_diff, function(x) mapply(function(x,y) x/y, x, scaling_factor, SIMPLIFY = FALSE) )
## output for final marginal effects
# coerce to a matrix
marginal_effects <- sapply(marginal_effects_scaled, function(x) sapply(x, function(x) as.matrix(x)))
# add names
colnames(marginal_effects) <- sapply(categories, function(x) paste("Category", x, sep = " "))
rownames(marginal_effects) <- colnames(X)
################################################################################################
########################### honest marginal effects are computed now ###########################
################################################################################################
### variance for the marginal effects
## compute prerequisities for variance of honest marginal effects
# scaling factor squared
scaling_factor_squared <- lapply(scaling_factor, function(x) x^2)
## compute the conditional means (predictions): already have this as forest_cond_means
# divide it by N to get the "mean"
forest_cond_means_mean <- lapply(forest_cond_means, function(x) lapply(x, function(x) x/n_data))
# calculate standard multiplication of weights and outcomes: honest_weights*y_ind_honest
forest_multi <- mapply(function(x,y) lapply(x, function(x) t(x)*y), forest_weights_diff_up_down, Y_ind_honest, SIMPLIFY = FALSE)
# subtract the mean from each obs i
forest_multi_demeaned <- mapply(function(x,y) mapply(function(x,y) x-matrix(y, nrow = nrow(x)), x, y, SIMPLIFY = FALSE), forest_multi, forest_cond_means_mean, SIMPLIFY = F)
## now do the single variances for each category m
# square the demeaned
forest_multi_demeaned_sq <- lapply(forest_multi_demeaned, function(x) lapply(x, function(x) x^2))
# sum all obs i together
forest_multi_demeaned_sq_sum <- lapply(forest_multi_demeaned_sq, function(x) lapply(x, function(x) sum(x)))
# divide by scaling factor
forest_multi_demeaned_sq_sum_scaled <- lapply(forest_multi_demeaned_sq_sum, function(x) mapply(function(x,y) x/y, x, scaling_factor_squared, SIMPLIFY = FALSE) )
# multiply by N/N-1 (normalize)
forest_multi_demeaned_sq_sum_scaled_norm <- lapply(forest_multi_demeaned_sq_sum_scaled, function(x) lapply(x, function(x) x*(n_data/(n_data-1)) ))
# put it into a shorter named object
variance <- forest_multi_demeaned_sq_sum_scaled_norm
## single variances done
## now compute the covariances
# multiply forest_var_multi_demeaned according to formula for covariance (shifted categories needed for computational convenience)
forest_multi_demeaned_0_last <- append(forest_multi_demeaned, list(rep(list(matrix(0, ncol = ncol(forest_multi_demeaned[[1]][[1]]), nrow = nrow(forest_multi_demeaned[[1]][[1]]))), ncol(X)))) # append zero matrix list
forest_multi_demeaned_0_first <- append(list(rep(list(matrix(0, ncol = ncol(forest_multi_demeaned[[1]][[1]]), nrow = nrow(forest_multi_demeaned[[1]][[1]]))), ncol(X))), forest_multi_demeaned) # prepend zero matrix list
# compute the multiplication of category m with m-1 according to the covariance formula
forest_multi_demeaned_cov <- mapply(function(x,y) mapply(function(x,y) x*y, x, y, SIMPLIFY = FALSE), forest_multi_demeaned_0_first, forest_multi_demeaned_0_last, SIMPLIFY = F)
# sum all obs i together
forest_multi_demeaned_cov_sum <- lapply(forest_multi_demeaned_cov, function(x) lapply(x, function(x) sum(x)))
# divide by scaling factor
forest_multi_demeaned_cov_sum_scaled <- lapply(forest_multi_demeaned_cov_sum, function(x) mapply(function(x,y) x/y, x, scaling_factor_squared, SIMPLIFY = FALSE) )
# multiply by N/N-1 (normalize)
forest_multi_demeaned_cov_sum_scaled_norm <- lapply(forest_multi_demeaned_cov_sum_scaled, function(x) lapply(x, function(x) x*(n_data/(n_data-1)) ))
# multiply by 2
forest_multi_demeaned_cov_sum_scaled_norm_mult2 <- lapply(forest_multi_demeaned_cov_sum_scaled_norm, function(x) lapply(x, function(x) x*2 ))
# put it into a shorter named object
covariance <- forest_multi_demeaned_cov_sum_scaled_norm_mult2
## covariances done
## put everything together according to the whole variance formula
# shift variances accordingly for ease of next computations (covariance already has the desired format)
variance_last <- append(variance, list(rep(list(0), ncol(X)))) # append zero element list
variance_first <- append(list(rep(list(0), ncol(X))), variance) # prepend zero element list
# put everything together according to formula: var_last + var_first - cov
variance_marginal_effects_final <- mapply(function(x,y,z) mapply(function(x,y,z) x+y-z, x, y, z, SIMPLIFY = FALSE), variance_last, variance_first, covariance, SIMPLIFY = F)
## output for final variances of marginal effects
# coerce to a matrix
variance_marginal_effects <- sapply(variance_marginal_effects_final, function(x) sapply(x, function(x) as.matrix(x)))
# add names
colnames(variance_marginal_effects) <- sapply(categories, function(x) paste("Category", x, sep = " "))
rownames(variance_marginal_effects) <- colnames(X)
## standard deviations
# take square root of variance
sd_marginal_effects <- sqrt(variance_marginal_effects)
#### z scores and p values ####
z_scores <- (marginal_effects)/(sd_marginal_effects)
# control for dividing zero by zero
z_scores[is.nan(z_scores)] = 0
# p values
p_values <- 2*pnorm(-abs(z_scores))
# put everzthing into a list of results
results <- list(marginal_effects, variance_marginal_effects, sd_marginal_effects, p_values)
names(results) <- c("MarginalEffects", "Variances", "StandardErrors", "pValues")
##################################################################################################
####### variances and standard deviations of the estimated honest marginal effects are done ######
##################################################################################################
}
# return results
return(results)
}
system.time(
ORF3 <- orf(X, Y, ntree, mtry, honesty, inference, margins)
)
round(ORF3$marginalEffects$MarginalEffects, 4)
coefstars(ORF3$marginalEffects$MarginalEffects, ORF3$marginalEffects$pValues)
system.time(
ORF3 <- orf(X, Y, ntree, mtry, honesty, inference, margins)
)
coefstars(ORF3$marginalEffects$MarginalEffects, ORF3$marginalEffects$pValues)
View(coefstars)
coefstars <- function(coefs, pvalues) {
#        coefs <- vector of estimated coefficients or fitted values/predictions
#        pvalues <- vector of pvalues for the corresponding coefficients
# check if coefs and pvalues have the same dimensions
if (all(dim(coefs))!=all(dim(pvalues))) {
stop("Dimensions of coefficients and p-values do not match. Programme temrinated.")
}
# chekc rownames of coefs
if (is.null(rownames(coefs))) {
coefs_rownames <- paste0("X", rep(1:nrow(coefs)))
} else {
coefs_rownames <- rownames(coefs)
}
# chekc colnames of coefs
if (is.null(rownames(coefs))) {
coefs_colnames <- paste0("Cat", rep(1:ncol(coefs)))
} else {
coefs_colnames <- colnames(coefs)
}
# generate stars (thanks to http://myowelt.blogspot.com/2008/04/beautiful-correlation-tables-in-r.html)
stars <- ifelse(pvalues < .001, "***", ifelse(pvalues < .01, "** ", ifelse(pvalues < .05, "*  ", "   ")))
# trunctuate the coefs to three decimals (take care of minus sign as well - "% .3f")
coefs <- apply(coefs, 2, function(x) format(sprintf("% .3f", round(x, 4))))
# paste effest with stars together
star_coefs <- sapply(seq_along(coefs[1, ]), function(i) as.matrix(paste0(coefs[, i], stars[, i])))
# add colnames and rownames
colnames(star_coefs) <- coefs_colnames
rownames(star_coefs) <- coefs_rownames
# print out table
output <- star_coefs
# return  output
return(output)
}
coefstars(ORF3$marginalEffects$MarginalEffects, ORF3$marginalEffects$pValues)
coefstars(ORF3$marginalEffects$MarginalEffects, ORF3$marginalEffects$pValues)
coefstars <- function(coefs, pvalues) {
#        coefs <- vector of estimated coefficients or fitted values/predictions
#        pvalues <- vector of pvalues for the corresponding coefficients
# check if coefs and pvalues have the same dimensions
if (all(dim(coefs))!=all(dim(pvalues))) {
stop("Dimensions of coefficients and p-values do not match. Programme temrinated.")
}
# chekc rownames of coefs
if (is.null(rownames(coefs))) {
coefs_rownames <- paste0("X", rep(1:nrow(coefs)))
} else {
coefs_rownames <- rownames(coefs)
}
# chekc colnames of coefs
if (is.null(rownames(coefs))) {
coefs_colnames <- paste0("Cat", rep(1:ncol(coefs)))
} else {
coefs_colnames <- colnames(coefs)
}
# generate stars (thanks to http://myowelt.blogspot.com/2008/04/beautiful-correlation-tables-in-r.html)
stars <- ifelse(pvalues < .001, "***", ifelse(pvalues < .01, "** ", ifelse(pvalues < .05, "*  ", "   ")))
# trunctuate the coefs to three decimals (take care of minus sign as well - "% .3f")
coefs <- apply(coefs, 2, function(x) format(sprintf("% .4f", round(x, 4))))
# paste effest with stars together
star_coefs <- sapply(seq_along(coefs[1, ]), function(i) as.matrix(paste0(coefs[, i], stars[, i])))
# add colnames and rownames
colnames(star_coefs) <- coefs_colnames
rownames(star_coefs) <- coefs_rownames
# print out table
output <- star_coefs
# return  output
return(output)
}
coefstars(ORF3$marginalEffects$MarginalEffects, ORF3$marginalEffects$pValues)
system.time(
ORF3 <- orf(X, Y, ntree, mtry, honesty, inference, margins)
)
coefstars(ORF3$marginalEffects$MarginalEffects, ORF3$marginalEffects$pValues)
system.time(
ORF3 <- orf(X, Y, ntree, mtry, honesty, inference, margins)
)
coefstars(ORF3$marginalEffects$MarginalEffects, ORF3$marginalEffects$pValues)
system.time(
ORF3 <- orf(X, Y, ntree, mtry, honesty, inference, margins)
)
coefstars(ORF3$marginalEffects$MarginalEffects, ORF3$marginalEffects$pValues)
## save the inputs:
inputs <- list(ntree, mtry, honesty, inference, margins)
names(inputs) <- c("ntree", "mtry", "honesty", "inference", "margins")
## save colnames
# Y - numeric response as only regression is supported (so far)
Y <- as.matrix(as.numeric(Y)) # make sure you can add colname
if (is.null(colnames(Y))) { colnames(Y) <- "Y" } # check if Y has name
Y_name <- colnames(Y) # save the name of Y
# X
if (is.null(colnames(X))) { colnames(X) <- paste0("X", rep(1:ncol(X))) } # check if X has name
X_name <- colnames(X) # save the name of X
## set needed dataframe and local variables
dat <- as.data.frame(cbind(Y, X)) # dataframe
colnames(dat) <- c(Y_name, X_name) # column names
n <- as.numeric(nrow(dat)) # number of observations
# parameters (categories)
categories <- as.numeric(sort(unique(Y))) # sequence of categories
ncat <- as.numeric(length(categories)) # number of categories
cat <- categories[1:(ncat-1)] # cat to esitmate / without the last category (not needed cuz P(Y_ind<=last_cat)=1)
load("~/Documents/HSG/ORF/gauss_comparison/data/train_data.rdata")
load("~/Documents/HSG/ORF/gauss_comparison/data/honest_data.rdata")
train_data <- temptem1
rows_train_data <- as.numeric(rownames(train_data)) # take rownames of train data as numeric
rows_train_data
Y_train <- as.matrix(train_data[, 1]) # take out Y train
Y_train
colnames(Y_train) <- Y_name # add column name
X_train <- train_data[, -1] # take out X
colnames(X_train) <- X_name # add column names
honest_data <- temptem2
rows_honest_data <- as.numeric(rownames(honest_data)) # take rownames of train data as numeric
rows_honest_data
Y_honest <- as.matrix(honest_data[, 1]) # take out Y train
colnames(Y_honest) <- Y_name # add column name
X_honest <- honest_data[, -1] # take out X
colnames(X_honest) <- X_name # add column names
## create variables needed for orf estimations
# create indicator variables (outcomes)
Y_ind_train <- lapply(cat, function(x) ifelse((Y_train <= x), 1, 0)) # train
Y_ind_honest <- lapply(cat, function(x) ifelse((Y_honest <= x), 1, 0))
# create dataset for ranger estimation
data_ind_train <- lapply(Y_ind_train, function(x) as.data.frame(cbind(as.matrix(unlist(x)), X_train)))
data_ind_honest <- lapply(Y_ind_honest, function(x) as.data.frame(cbind(as.matrix(unlist(x)), X_honest)))
# estimate ncat-1 forests (everything on the same data: placing splits and effect estimation), no subsampling
forest <- lapply(data_ind_train, function(x) ranger(dependent.variable.name = paste(Y_name), data = x,
num.trees = ntree, mtry = mtry, replace = FALSE,
sample.fraction = 0.5, importance = "none"))
# get honest weights
forest_weights <- mapply(function(x,y,z) get_forest_weights(x, y, z), forest, data_ind_honest, data_ind_train, SIMPLIFY = F)
honest_weights <- lapply(forest_weights, function(x) x[rows_honest_data, ]) # take out honest sample honest weights
train_weights <- lapply(forest_weights, function(x) x[rows_train_data, ]) # take out train sample honest weights
## make honest predictions, i.e. fitted values based on honest sample
# honest sample predictions
honest_pred <- mapply(function(x,y) as.matrix(x %*% y[, 1]), honest_weights, data_ind_honest, SIMPLIFY = F) # honest weights for honest data
#rownames(honest_pred) <- rows_honest_data
# train sample predictions
train_pred <- mapply(function(x,y) as.matrix(x %*% y[, 1]), train_weights, data_ind_honest, SIMPLIFY = F) # honest weights for train data
#rownames(train_pred) <- rows_train_data
# put the prediction together for whole sample and order them as original data
forest_pred <- mapply(function(x,y) rbind(x, y), honest_pred, train_pred, SIMPLIFY = F)
# sort according to rownames
forest_pred <- lapply(forest_pred, function(x) as.numeric(x[order(as.numeric(row.names(x))), ]))
# add the probability for the last outcome (always 1)
pred_1 <- append(forest_pred, list(rep(1, n)))
# prepend zero vector to predictions for later differencing
pred_0 <- append(list(rep(0, n)), forest_pred) # append a first 0 elemnt for the list
# total predictions (make sure it returns a list)
pred_total <- as.list(mapply(function(x,y) x-y, pred_1, pred_0, SIMPLIFY = F))
# avoid negative predictions
pred_total <- lapply(pred_total, function(x) ifelse((x < 0), 0, x))
# coerce to final matrix
pred_total <- sapply(pred_total, function(x) as.matrix(x))
# normalize predictions
pred_final <- matrix(apply(pred_total, 1, function(x) (x)/(sum(x))), ncol = ncat, byrow = T)
# add names
colnames(pred_final) <- sapply(categories, function(x) paste("Category", x, sep = " "))
# compute OOB MSE based on whole sample
honest_mse <- mse(pred_final, Y)
# compute OOB RPS based on whole sample
honest_rps <- rps(pred_final, Y)
honest_mse
honest_rps
## convert probabilities into class predictions ("classification")
pred_class <- as.matrix(apply(pred_final, 1, which.max))
colnames(pred_class) <- "Category"
pred_class
forest_pred
rownames(forest_pred)
# compute the variances for the categorical predictions
var_final <- get_orf_variance(honest_pred, honest_weights, train_pred, train_weights, Y_ind_honest)
data_ind_honest
data <- data_ind_honest
honesty
inference
forest
# get number of observations
n_data <- nrow(data[[1]])
# get X as matrix
X <- as.matrix(data[[1]][, -(1)])
# get categories
categories <- seq(1:(length(data)+1))
# evaluate at mean always here in the main orf function
eval <- "median"
# take out honest indicator Ys
Y_ind_honest <- lapply(data, function(x) as.matrix(x[, 1]))
h_std <- 1
# check if X is continuous or dummy or categorical
X_type <- apply(X, 2, function(x) length(unique(x)))
# now determine the type of X
X_continuous <- which(X_type > 10) # define IDs of continuous Xs
X_dummy <- which(X_type == 2) # define IDs of dummies
X_categorical <- which(X_type > 2 & X_type <= 10)
# additional check for constant variables which are nonsensical
if (any(X_type == 1) | any(X_type == 0)) {
stop("Some of the covariates are constant. This is non-sensical for evaluation of marginal effects. Programme terminated.")
}
# decide if the marginal effects should be computed at mean or at median
if (eval=="mean") {
# variable of interest: X_1 to X_last, ME at mean
X_mean <- lapply(1:ncol(X), function(x) colMeans(X)) # set all Xs to their mean values (so many times as we have Xs)
} else if (eval=="median") {
# variable of interest: X_1 to X_last, ME at median
X_mean <- lapply(1:ncol(X), function(x) apply(X, 2, median)) # set all Xs to their median values (so many times as we have Xs)
} else {
stop("Incorrect evaluation point. Programme terminated.")
}
# get SD of Xs
X_sd <- apply(X, 2, sd)
# create X_up (X_mean + 0.1 * X_sd)
X_up <- X_mean[[1]] + h_std*X_sd
# create X_down (X_mean - 0.1 * X_sd)
X_down <- X_mean[[1]] - h_std*X_sd
## now check for the support of X
# check X_max
X_max <- apply(X, 2, max)
# check X_min
X_min <- apply(X, 2, min)
# check if X_up is within the range X_min and X_max
X_up <- (X_up < X_max) * X_up + (X_up >= X_max) * X_max
X_up <- (X_up > X_min) * X_up + (X_up <= X_min) * (X_min + h_std * X_sd)
# check if X_down is within the range X_min and X_max
X_down <- (X_down > X_min) * X_down + (X_down <= X_min) * X_min
X_down <- (X_down < X_max) * X_down + (X_down >= X_max) * (X_max - h_std * X_sd)
## now we need 2 datasets: one with X_up and second with X_down
# X_mean_up
X_mean_up <- lapply(seq_along(X_mean), function(i) replace(X_mean[[i]], i, X_up[i]) )
# X_mean_down
X_mean_down <- lapply(seq_along(X_mean), function(i) replace(X_mean[[i]], i, X_down[i]) )
# adjust for categorical X (works also for zero categorical)
for (i in X_categorical) {
X_mean_up[[i]][[i]] <- ceiling(mean(X[,i]))
X_mean_down[[i]][[i]] <- floor(mean(X[,i]))
}
# adjust for dummies (works also for zero dummies)
for (i in X_dummy) {
X_mean_up[[i]][[i]] <- max(X[,i])
X_mean_down[[i]][[i]] <- min(X[,i])
}
# extract weights for desired Xs up: get weights from honest sample and predict weights for evaluation points from HONEST sample
forest_weights_up <- predict_forest_weights_for_ME(forest, X, X_mean_up)
# extract weights for desired Xs down
forest_weights_down <- predict_forest_weights_for_ME(forest, X, X_mean_down)
# now subtract the weights according to the ME formula
forest_weights_diff_up_down <- mapply(function(x,y) mapply(function(x,y) x-y, x, y,  SIMPLIFY = F), forest_weights_up, forest_weights_down, SIMPLIFY = F)
## compute prerequisities for marginal effects
# compute the conditional means: weights%*%y (predictions are based on honest sample)
forest_cond_means <- mapply(function(x,y) lapply(x, function(x) x%*%y), forest_weights_diff_up_down, Y_ind_honest, SIMPLIFY = FALSE)
# subtract conditional means according to formula to isolate categories
forest_cond_means_0_last <- append(forest_cond_means, list(rep(list(0), ncol(X)))) # append zero elemnt list
forest_cond_means_0_first <- append(list(rep(list(0), ncol(X))), forest_cond_means) # prepend zero element list
# compute the scaling factor: X_up-X_down=2*X_sd
scaling_factor <- as.list(X_up - X_down)
# set scaling factor to zero for categorical and dummy variables
for (i in X_categorical & X_dummy) {
scaling_factor[[i]] <- 0
}
# now compute the differences for marginal effects
marginal_effects_diff <- mapply(function(x,y) mapply(function(x,y) x-y, x, y, SIMPLIFY = F), forest_cond_means_0_last, forest_cond_means_0_first, SIMPLIFY = F)
# scale marginal effects
marginal_effects_scaled <- lapply(marginal_effects_diff, function(x) mapply(function(x,y) x/y, x, scaling_factor, SIMPLIFY = FALSE) )
## output for final marginal effects
# coerce to a matrix
marginal_effects <- sapply(marginal_effects_scaled, function(x) sapply(x, function(x) as.matrix(x)))
# add names
colnames(marginal_effects) <- sapply(categories, function(x) paste("Category", x, sep = " "))
rownames(marginal_effects) <- colnames(X)
marginal_effects
round(marginal_effects, 4)
### variance for the marginal effects
## compute prerequisities for variance of honest marginal effects
# scaling factor squared
scaling_factor_squared <- lapply(scaling_factor, function(x) x^2)
## compute the conditional means (predictions): already have this as forest_cond_means
# divide it by N to get the "mean"
forest_cond_means_mean <- lapply(forest_cond_means, function(x) lapply(x, function(x) x/n_data))
# calculate standard multiplication of weights and outcomes: honest_weights*y_ind_honest
forest_multi <- mapply(function(x,y) lapply(x, function(x) t(x)*y), forest_weights_diff_up_down, Y_ind_honest, SIMPLIFY = FALSE)
# subtract the mean from each obs i
forest_multi_demeaned <- mapply(function(x,y) mapply(function(x,y) x-matrix(y, nrow = nrow(x)), x, y, SIMPLIFY = FALSE), forest_multi, forest_cond_means_mean, SIMPLIFY = F)
## now do the single variances for each category m
# square the demeaned
forest_multi_demeaned_sq <- lapply(forest_multi_demeaned, function(x) lapply(x, function(x) x^2))
# sum all obs i together
forest_multi_demeaned_sq_sum <- lapply(forest_multi_demeaned_sq, function(x) lapply(x, function(x) sum(x)))
# divide by scaling factor
forest_multi_demeaned_sq_sum_scaled <- lapply(forest_multi_demeaned_sq_sum, function(x) mapply(function(x,y) x/y, x, scaling_factor_squared, SIMPLIFY = FALSE) )
# multiply by N/N-1 (normalize)
forest_multi_demeaned_sq_sum_scaled_norm <- lapply(forest_multi_demeaned_sq_sum_scaled, function(x) lapply(x, function(x) x*(n_data/(n_data-1)) ))
# put it into a shorter named object
variance <- forest_multi_demeaned_sq_sum_scaled_norm
## now compute the covariances
# multiply forest_var_multi_demeaned according to formula for covariance (shifted categories needed for computational convenience)
forest_multi_demeaned_0_last <- append(forest_multi_demeaned, list(rep(list(matrix(0, ncol = ncol(forest_multi_demeaned[[1]][[1]]), nrow = nrow(forest_multi_demeaned[[1]][[1]]))), ncol(X)))) # append zero matrix list
forest_multi_demeaned_0_first <- append(list(rep(list(matrix(0, ncol = ncol(forest_multi_demeaned[[1]][[1]]), nrow = nrow(forest_multi_demeaned[[1]][[1]]))), ncol(X))), forest_multi_demeaned) # prepend zero matrix list
# compute the multiplication of category m with m-1 according to the covariance formula
forest_multi_demeaned_cov <- mapply(function(x,y) mapply(function(x,y) x*y, x, y, SIMPLIFY = FALSE), forest_multi_demeaned_0_first, forest_multi_demeaned_0_last, SIMPLIFY = F)
# sum all obs i together
forest_multi_demeaned_cov_sum <- lapply(forest_multi_demeaned_cov, function(x) lapply(x, function(x) sum(x)))
# divide by scaling factor
forest_multi_demeaned_cov_sum_scaled <- lapply(forest_multi_demeaned_cov_sum, function(x) mapply(function(x,y) x/y, x, scaling_factor_squared, SIMPLIFY = FALSE) )
# multiply by N/N-1 (normalize)
forest_multi_demeaned_cov_sum_scaled_norm <- lapply(forest_multi_demeaned_cov_sum_scaled, function(x) lapply(x, function(x) x*(n_data/(n_data-1)) ))
# multiply by 2
forest_multi_demeaned_cov_sum_scaled_norm_mult2 <- lapply(forest_multi_demeaned_cov_sum_scaled_norm, function(x) lapply(x, function(x) x*2 ))
# put it into a shorter named object
covariance <- forest_multi_demeaned_cov_sum_scaled_norm_mult2
## put everything together according to the whole variance formula
# shift variances accordingly for ease of next computations (covariance already has the desired format)
variance_last <- append(variance, list(rep(list(0), ncol(X)))) # append zero element list
variance_first <- append(list(rep(list(0), ncol(X))), variance) # prepend zero element list
# put everything together according to formula: var_last + var_first - cov
variance_marginal_effects_final <- mapply(function(x,y,z) mapply(function(x,y,z) x+y-z, x, y, z, SIMPLIFY = FALSE), variance_last, variance_first, covariance, SIMPLIFY = F)
## output for final variances of marginal effects
# coerce to a matrix
variance_marginal_effects <- sapply(variance_marginal_effects_final, function(x) sapply(x, function(x) as.matrix(x)))
# add names
colnames(variance_marginal_effects) <- sapply(categories, function(x) paste("Category", x, sep = " "))
rownames(variance_marginal_effects) <- colnames(X)
## standard deviations
# take square root of variance
sd_marginal_effects <- sqrt(variance_marginal_effects)
#### z scores and p values ####
z_scores <- (marginal_effects)/(sd_marginal_effects)
# control for dividing zero by zero
z_scores[is.nan(z_scores)] = 0
# p values
p_values <- 2*pnorm(-abs(z_scores))
# put everzthing into a list of results
results <- list(marginal_effects, variance_marginal_effects, sd_marginal_effects, p_values)
names(results) <- c("MarginalEffects", "Variances", "StandardErrors", "pValues")
p_values
round(p_values, 3)
coefstars(marginal_effects, p_values)
coefstars(ORF3$marginalEffects$MarginalEffects, ORF3$marginalEffects$pValues)
library(devtools)
document()
document()
check()
library(orf)
document()
check()
document()
check()
document()
check()
document()
check()
document()
document()
check()
library(orf)
document()
check()
document()
check()
library(orf)
