}
# adjust for dummies (works also for zero dummies)
for (i in X_dummy) {
X_mean_up[[i]][, i] <- max(X[, i])
X_mean_down[[i]][, i] <- min(X[, i])
}
# compare speed of weights with vectorization
# old weights
system.time(
forest_weights_up <- predict_forest_weights_for_ME(forest$trainForests, X, X_mean_up)
)
forest <- forest$trainForests
data <- X
pred_data <- X_mean_up
# extract weights for desired Xs up
leaf_IDs <- lapply(forest, function(x) {
# first get terminal nodes, i.e get terminal nodes for each obs and each tree
# run your (new) data through the forest structure and look where your observations end up
leaf_IDs <- predict(x, data, type = "terminalNodes")$predictions
# put leaf_IDs into a list (one element for one tree)
lapply(seq_along(leaf_IDs[1, ]), function(i) leaf_IDs[, i])
})
# get the leaf size as counts of observations in leaves
leaf_size <- lapply(leaf_IDs, function(x) {
lapply(x, function(x) ave(x, x, FUN = length))
})
# start looping over all X_means
leaf_IDs_pred <- lapply(forest, function(x) {
# loop over Xs
lapply(seq_along(pred_data), function(X_index) {
# now do the same for your prediction data
leaf_IDs_pred <- predict(x, as.matrix(pred_data[[X_index]]), type = "terminalNodes")$predictions
# put leaf_IDs into a list
lapply(seq_along(leaf_IDs_pred[1, ]), function(i) leaf_IDs_pred[, i])
})
})
forest_index <- 1
X_indez <- 1
# compare speed of weights with vectorization
# old weights
system.time(
old_weights <- pred_weights_C(leaf_IDs_pred[[forest_index]][[X_index]], leaf_IDs[[forest_index]], leaf_size[[forest_index]])
)
X_index <- 1
# compare speed of weights with vectorization
# old weights
system.time(
old_weights <- pred_weights_C(leaf_IDs_pred[[forest_index]][[X_index]], leaf_IDs[[forest_index]], leaf_size[[forest_index]])
)
leaf_IDs_pred[[1]]
View(leaf_IDs_pred)
View(leaf_IDs)
leaf_IDs[[1]]
leaf_IDs_pred[[forest_index]][[X_index]]
leaf_IDs_pred[[forest_index]][[X_index]][[1]]
leaf_IDs[[forest_index]][[1]]
leaf_IDs_pred_1 <- leaf_IDs_pred[[forest_index]][[X_index]][[1]]
leaf_IDs_1 <- leaf_IDs[[forest_index]][[1]]
leaf_size_1 <- leaf_size[[forest_index]][[1]]
leaf_size_1
tree_out <- matrix(NA, 500, 500)
for(i in 1:500) {
for(j in 1:500) {
tree_out[i,j] = leaf_IDs_pred_1[i]==leaf_IDs_1[j];
tree_out[i,j] = tree_out[i,j]/leaf_size_1[j];
}
}
head(tree_out)
leaf_IDs_pred_1
leaf_IDs_1
leaf_IDs_pred_1 <- leaf_IDs_pred[[forest_index]][[X_index]][[1]]
leaf_IDs_1 <- leaf_IDs[[forest_index]][[1]]
leaf_IDs_pred_1
leaf_IDs_1
all.equal(leaf_IDs_1, leaf_IDs_pred_1)
all(leaf_IDs_1 == leaf_IDs_pred_1)
(leaf_IDs_1 - leaf_IDs_pred_1)
rowSums(tree_out)
i <- 1
leaf_IDs_pred_1[i]
leaf_IDs_1
leaf_IDs_pred_1[i] == leaf_IDs_1
rep(leaf_IDs_pred_1[i], 500)
leaf_IDs_1
rep(leaf_IDs_pred_1[i], 500) == leaf_IDs_1
(rep(leaf_IDs_pred_1[i], 500) == leaf_IDs_1)/leaf_size_1
tree_out[i,j]
tree_out[1,]
all(tree_out[1,] == ( (rep(leaf_IDs_pred_1[i], 500) == leaf_IDs_1)/leaf_size_1))
tree_out_new <- matrix(NA, 500, 500)
for(i in 1:500) {
#for(j in 1:500) {
tree_out_new[i, ] <- (rep(leaf_IDs_pred_1[i], 500) == leaf_IDs_1)/leaf_size_1
#tree_out[i,j] = leaf_IDs_pred_1[i]==leaf_IDs_1[j];
#tree_out[i,j] = tree_out[i,j]/leaf_size_1[j];
#}
}
all(tree_out==tree_out_new)
library(Rcpp)
library(Rcpp)
# cpp function
cppFunction('NumericVector replicate(int x, int y) {
NumericVector leaf_IDs_pred_vec = rep_len(x, y);
return leaf_IDs_pred_vec;
}')
# cpp function
cppFunction('NumericVector replicate(int x, int y) {
NumericVector leaf_IDs_pred_vec = rep_each(x, y);
return leaf_IDs_pred_vec;
}')
Rcpp::sourceCpp('~/Documents/HSG/ORF/Rcpp/rep_each.cpp')
Rcpp::sourceCpp('~/Documents/HSG/ORF/Rcpp/rep_each.cpp')
rep_example(1,10)
rep_example(1,100)
rep_example(0.5,100)
Rcpp::sourceCpp('~/Documents/HSG/ORF/Rcpp/rep_each.cpp')
Rcpp::sourceCpp('~/Documents/HSG/ORF/Rcpp/rep_each.cpp')
Rcpp::sourceCpp('~/Documents/HSG/ORF/Rcpp/rep_each.cpp')
Rcpp::sourceCpp('~/Documents/HSG/ORF/Rcpp/rep_each.cpp')
Rcpp::sourceCpp('~/Documents/HSG/ORF/Rcpp/rep_each.cpp')
# new weights
system.time(
new_weights <- vectorized_pred_weights_C(leaf_IDs_pred[[forest_index]][[X_index]], leaf_IDs[[forest_index]], leaf_size[[forest_index]])
)
all(new_weights==old_weights)
devtools::load_all()
load("~/Documents/HSG/ORF/all_functions/sim_data.RData")
X <- as.matrix(odata[, 2:16])
X_test <- as.matrix(odata_test[, 2:16])
Y <- as.matrix(odata[, 1])
ntree <- 1000
mtry <- 5
nmin <- 5
honesty <- FALSE
inference <- FALSE
margins <- FALSE
honesty <- TRUE
inference <- FALSE
margins <- FALSE
# estimate orf in first place
set.seed(311992) # put your birthday here
# estimate model
orf_model <- orf(X,Y,ntree,mtry,nmin,honesty,inference,margins)
orf_model$forestInfo$inputs$inference <- TRUE
forest <- orf_model
eval = "mean"
newdata <- NULL
### decide if prediction or in sample marginal effects should be evaluated
if (is.null(newdata)) {
# if no newdata supplied, estimate in sample marginal effects
if (forest$forestInfo$inputs$honesty == FALSE) {
data <- forest$forestInfo$trainData # take in-sample data
} else if (forest$forestInfo$inputs$honesty == TRUE) {
data <- forest$forestInfo$honestData
}
} else {
# check if newdata is compatible with train data
if (ncol(newdata) != ncol(forest$forestInfo$trainData)) {
stop("newdata is not compatible with training data. Programme terminated.")
} else {
data = newdata
}
}
### data checks done
# ----------------------------------------------------------------------------------- #
### data preparation and checks
# get number of observations
n_data <- as.numeric(nrow(data))
# get categories
categories <- forest$forestInfo$categories
# get X as matrix
X <- as.matrix(data[, -1])
# get Y as matrix
Y <- as.matrix(data[, 1])
# create indicator variables (outcomes)
Y_ind <- lapply(categories[1:length(categories)-1], function(x) ifelse((Y <= x), 1, 0))
# create datasets with indicator outcomes
data_ind <- lapply(Y_ind, function(x) as.data.frame(cbind(as.matrix(unlist(x)), X)))
# ----------------------------------------------------------------------------------- #
### marginal effects preparation
# share of SD to be used
h_std <- 0.1
# check if X is continuous or dummy or categorical
X_type <- apply(X, 2, function(x) length(unique(x)))
# now determine the type of X
X_continuous <- which(X_type > 10) # define IDs of continuous Xs
X_dummy <- which(X_type == 2) # define IDs of dummies
X_categorical <- which(X_type > 2 & X_type <= 10)
# additional check for constant variables which are nonsensical
if (any(X_type == 1) | any(X_type == 0)) {
stop("Some of the covariates are constant. This is non-sensical for evaluation of marginal effects. Programme terminated.")
}
# ----------------------------------------------------------------------------------- #
### check the evaluation point
if (eval == "atmean") {
# variable of interest: X_1 to X_last, ME at mean
X_mean <- lapply(1:ncol(X), function(x) t(as.matrix(colMeans(X)))) # set all Xs to their mean values (so many times as we have Xs)
} else if (eval == "atmedian") {
# variable of interest: X_1 to X_last, ME at median
X_mean <- lapply(1:ncol(X), function(x) t(as.matrix(apply(X, 2, median)))) # set all Xs to their median values (so many times as we have Xs)
} else if (eval == "mean") {
# # variable of interest: X_1 to X_last, mean ME
X_mean <- lapply(1:ncol(X), function(x) X) # set all Xs to their exact values (so many times as we have Xs)
} else {
stop("Incorrect evaluation point. Programme terminated.")
}
# ----------------------------------------------------------------------------------- #
### get data needed for evaluation of ME
# get number of evaluation points
X_rows <- nrow(X_mean[[1]])
# get number of Xs
X_cols <- ncol(X_mean[[1]])
# get SD of Xs
X_sd <- rep_row(apply(X, 2, sd), n = X_rows)
# create X_up (X_mean + 0.1 * X_sd)
X_up <- X_mean[[1]] + h_std*X_sd
# create X_down (X_mean - 0.1 * X_sd)
X_down <- X_mean[[1]] - h_std*X_sd
## now check for the support of X
# check X_max
X_max <- rep_row(apply(X, 2, max), n = X_rows)
# check X_min
X_min <- rep_row(apply(X, 2, min), n = X_rows)
# check if X_up is within the range X_min and X_max
X_up <- (X_up < X_max) * X_up + (X_up >= X_max) * X_max
X_up <- (X_up > X_min) * X_up + (X_up <= X_min) * (X_min + h_std * X_sd)
# check if X_down is within the range X_min and X_max
X_down <- (X_down > X_min) * X_down + (X_down <= X_min) * X_min
X_down <- (X_down < X_max) * X_down + (X_down >= X_max) * (X_max - h_std * X_sd)
# check if X_up and X_down are same
if (any(X_up == X_down)) {
# adjust to higher share of SD
X_up   <- (X_up > X_down) * X_up   + (X_up == X_down) * (X_up   + 0.5 * h_std * X_sd)
X_down <- (X_up > X_down) * X_down + (X_up == X_down) * (X_down - 0.5 * h_std * X_sd)
# check the min max range again
X_up   <- (X_up < X_max) * X_up + (X_up >= X_max) * X_max
X_down <- (X_down > X_min) * X_down + (X_down <= X_min) * X_min
}
# checks for support of X done
# ----------------------------------------------------------------------------------- #
## now we need 2 datasets: one with X_up and second with X_down
# X_mean_up all (continous)
X_mean_up <- X_mean
X_mean_down <- X_mean
# replace values accordingly
for (i in 1:X_cols) {
X_mean_up[[i]][, i] <- X_up[, i]
X_mean_down[[i]][, i] <- X_down[, i]
}
# adjust for categorical X (works also for zero categorical)
for (i in X_categorical) {
X_mean_up[[i]][, i] <- ceiling(mean(X[, i]))
X_mean_down[[i]][, i] <- floor(mean(X[, i]))
}
# adjust for dummies (works also for zero dummies)
for (i in X_dummy) {
X_mean_up[[i]][, i] <- max(X[, i])
X_mean_down[[i]][, i] <- min(X[, i])
}
forest <- forest$trainForests
data <- X
pred_data <- X_mean_up
# extract weights for desired Xs up
leaf_IDs <- lapply(forest, function(x) {
# first get terminal nodes, i.e get terminal nodes for each obs and each tree
# run your (new) data through the forest structure and look where your observations end up
leaf_IDs <- predict(x, data, type = "terminalNodes")$predictions
# put leaf_IDs into a list (one element for one tree)
lapply(seq_along(leaf_IDs[1, ]), function(i) leaf_IDs[, i])
})
# get the leaf size as counts of observations in leaves
leaf_size <- lapply(leaf_IDs, function(x) {
lapply(x, function(x) ave(x, x, FUN = length))
})
# start looping over all X_means
leaf_IDs_pred <- lapply(forest, function(x) {
# loop over Xs
lapply(seq_along(pred_data), function(X_index) {
# now do the same for your prediction data
leaf_IDs_pred <- predict(x, as.matrix(pred_data[[X_index]]), type = "terminalNodes")$predictions
# put leaf_IDs into a list
lapply(seq_along(leaf_IDs_pred[1, ]), function(i) leaf_IDs_pred[, i])
})
})
forest_index <- 1
X_index <- 1
# compare speed of weights with vectorization
# old weights
system.time(
old_weights <- pred_weights_C(leaf_IDs_pred[[forest_index]][[X_index]], leaf_IDs[[forest_index]], leaf_size[[forest_index]])
)
Rcpp::sourceCpp('~/Documents/HSG/ORF/Rcpp/rep_each.cpp')
# new weights
system.time(
new_weights <- vectorized_pred_weights_C(leaf_IDs_pred[[forest_index]][[X_index]], leaf_IDs[[forest_index]], leaf_size[[forest_index]])
)
Rcpp::sourceCpp('~/Documents/HSG/ORF/Rcpp/rep_each.cpp')
# new weights
system.time(
new_weights <- vectorized_pred_weights_C(leaf_IDs_pred[[forest_index]][[X_index]], leaf_IDs[[forest_index]], leaf_size[[forest_index]])
)
Rcpp::sourceCpp('~/Documents/HSG/ORF/Rcpp/rep_each.cpp')
system.time(
forest_weights_up <- lapply(seq_along(forest), function(forest_index) {
# go through forests and Xs
lapply(seq_along(pred_data), function(X_index) {
pred_weights_C(leaf_IDs_pred[[forest_index]][[X_index]], leaf_IDs[[forest_index]], leaf_size[[forest_index]])
})
}))
forest_weights_up_old <- forest_weights_up
system.time(
forest_weights_up <- lapply(seq_along(forest), function(forest_index) {
# go through forests and Xs
lapply(seq_along(pred_data), function(X_index) {
vectorized_pred_weights_C(leaf_IDs_pred[[forest_index]][[X_index]], leaf_IDs[[forest_index]], leaf_size[[forest_index]])
})
}))
Rcpp::sourceCpp('~/Documents/HSG/ORF/Rcpp/rep_each.cpp')
system.time(
forest_weights_up <- lapply(seq_along(forest), function(forest_index) {
# go through forests and Xs
lapply(seq_along(pred_data), function(X_index) {
vectorized_pred_weights_C(leaf_IDs_pred[[forest_index]][[X_index]], leaf_IDs[[forest_index]], leaf_size[[forest_index]])
})
}))
Rcpp::sourceCpp('~/Documents/HSG/ORF/Rcpp/integer_weights.cpp')
system.time(
forest_weights_up <- lapply(seq_along(forest), function(forest_index) {
# go through forests and Xs
lapply(seq_along(pred_data), function(X_index) {
int_pred_weights_C(leaf_IDs_pred[[forest_index]][[X_index]], leaf_IDs[[forest_index]], leaf_size[[forest_index]])
})
}))
system.time(
forest_weights_up_old <- lapply(seq_along(forest), function(forest_index) {
# go through forests and Xs
lapply(seq_along(pred_data), function(X_index) {
pred_weights_C(leaf_IDs_pred[[forest_index]][[X_index]], leaf_IDs[[forest_index]], leaf_size[[forest_index]])
})
}))
Rcpp::sourceCpp('~/Documents/HSG/ORF/Rcpp/integer_weights.cpp')
forest_weights_up[[1]][[1]]-forest_weights_up_old[[1]][[1]]
forest_weights_up[[1]][[1]]==forest_weights_up_old[[1]][[1]]
all(forest_weights_up[[1]][[1]]==forest_weights_up_old[[1]][[1]])
system.time(
forest_weights_up <- lapply(seq_along(forest), function(forest_index) {
# go through forests and Xs
lapply(seq_along(pred_data), function(X_index) {
int_pred_weights_C(leaf_IDs_pred[[forest_index]][[X_index]], leaf_IDs[[forest_index]], leaf_size[[forest_index]])
})
}))
# compare speed of weights with vectorization
# old weights
system.time(
old_weights <- pred_weights_C(leaf_IDs_pred[[forest_index]][[X_index]], leaf_IDs[[forest_index]], leaf_size[[forest_index]])
)
# new weights
system.time(
new_weights <- vectorized_pred_weights_C(leaf_IDs_pred[[forest_index]][[X_index]], leaf_IDs[[forest_index]], leaf_size[[forest_index]])
)
# new weights
system.time(
new_weights <- int_pred_weights_C(leaf_IDs_pred[[forest_index]][[X_index]], leaf_IDs[[forest_index]], leaf_size[[forest_index]])
)
install.packages("microbenchmark")
library(microbenchmark)
microbenchmark(pred_weights_C(leaf_IDs_pred[[forest_index]][[X_index]], leaf_IDs[[forest_index]], leaf_size[[forest_index]]),
vectorized_pred_weights_C(leaf_IDs_pred[[forest_index]][[X_index]], leaf_IDs[[forest_index]], leaf_size[[forest_index]]),
int_pred_weights_C(leaf_IDs_pred[[forest_index]][[X_index]], leaf_IDs[[forest_index]], leaf_size[[forest_index]]))
microbenchmark(pred_weights_C(leaf_IDs_pred[[forest_index]][[X_index]], leaf_IDs[[forest_index]], leaf_size[[forest_index]]),
vectorized_pred_weights_C(leaf_IDs_pred[[forest_index]][[X_index]], leaf_IDs[[forest_index]], leaf_size[[forest_index]]),
int_pred_weights_C(leaf_IDs_pred[[forest_index]][[X_index]], leaf_IDs[[forest_index]], leaf_size[[forest_index]]), times = 10)
546/3
309 + ((309/2)/2)
351/2
293 + 175
# load orf
library(orf)
# estimate with polr
library(MASS)
library(erer)
# set wd
setwd("/home/okasag/Documents/HSG/ORF/real_data_app")
source("~/Documents/HSG/ORF/real_data_app/marginal_effects/output_ologit.R")
# get data
### 1. WINE DATA
load("~/Documents/HSG/ORF/data/winequality_dataset_rdata.Rdata")
summary(dataset)
summary(dataset$density)
head(dataset$density)
# load libriaries
library(party)
install.packages("party")
# load libriaries
library(party)
# dataset
set.seed(3192) # put your birthday here
# forest settings
controlforest <- cforest_control(mtry = 5, ntree = 1, mincriterion = 0) # controls for horniks forest
# estimate the forest
forest <- cforest(y ~ ., data = dataset, controls = controlforest)
controlforest
forest
# estimate the forest
forest <- cforest(as.ordered(y) ~ ., data = dataset, controls = controlforest)
forest
# plot the tree
plot(forest)
# summary the tree
summary(forest)
forest
# load libriaries
library(party)
# dataset
data("mammoexp", package = "TH.data")
mtree <- ctree(ME ~ ., data = mammoexp)
plot(mtree)
summary(mtree)
mtree
mtree@responses
mtree@tree
mtree@cond_distr_response()
mtree@predict_response()
mtree@
mtree@cond_distr_response()
mtree@cond_distr_response()
mtree@tree
mtree@tree[1]
nodes(mtree)
nodes(mtree, 1)
where(mtree)
# check where the observations fall in (nodeIDs)
nodeIDs <- where(mtree)
# check where the observations fall in (nodeIDs)
nodeIDs <- as.numeric(where(mtree))
nodeIDs
which(nodeIDs == 2)
node2 <- which(nodeIDs == 2)
# which obs fall in leaf 2
node2 <- which(nodeIDs == 2)
# compute probabilities for node 2
mammoexp$ME
# compute probabilities for node 2
mammoexp$ME[1,]
# compute probabilities for node 2
mammoexp$ME[1]
# compute probabilities for node 2
mammoexp$ME[node2]
# compute probabilities for node 2
node2_y <- mammoexp$ME[node2]
# now probabilities
node2_y == "Never"
# now probabilities
which(node2_y == "Never")
# how many within node 2 are "Never"
node2_never <- length(which(node2_y == "Never"))
node2_never
node2_wyear <- length(which(node2_y == "Within a Year"))
node2_oyear <- length(which(node2_y == "Over a Year"))
node2_all <- length(node2_y)
# compute probabilities
p_never <- node2_never/node2_all
p_wyear <- node2_wyear/node2_all
p_oyear <- node2_oyear/node2_all
p_wyear <- node2_wyear/node2_all
p_oyear <- node2_oyear/node2_all
# check if they sum up to 1
(p_never + p_wyear + p_oyear) == 1
p_never
p_wyear
p_oyear
mtree@cond_distr_response()
node2
mtree@cond_distr_response(2)
mtree@cond_distr_response()[2]
p_never
p_wyear
p_oyear
# ordinal forest
library(ordinalForest)
install.packages("ordinalForest")
install.packages("ordinalForest")
# ordinal forest
library(ordinalForest)
# estimate
ordinalf <- ordfor(depvar="ME", data=mammoexp, nsets=1000, ntreeperdiv=100, ntreefinal=1, perffunction = "equal")
ordinalf
summary(ordinalf)
ordinalf$forestfinal
ordinalf$bordersbest
ordinalf$forests
ordinalf$perffunctionvalues
ordinalf$bordersb
ordinalf$classes
ordinalf$nsets
ordinalf$classimp
ordinalf$classfreq
predict(ordinalf)
predict(ordinalf, newdata = mammoexp)
predict(ordinalf, newdata = mammoexp[-c(ME)])
predict(ordinalf, newdata = mammoexp[-c("ME")])
ordinalf$forestfinal$predictions
ordinalf$forestfinal$treetype
ordinalf$forestfinal$mtry
predict(ordinalf, newdata = mammoexp[,2:6])
predict(ordinalf, newdata = mammoexp)
# estimate
ordinalf <- ordfor(depvar="ME", data=mammoexp, nsets=1000, ntreeperdiv=100, ntreefinal=2, perffunction = "equal")
summary(ordinalf)
predict(ordinalf, newdata = mammoexp)
ordinalfpred <- predict(ordinalf, newdata = mammoexp)
ordinalf$forestfinal$predictions
