@article{Okasa2019,
title={Random Forest Estimation of the Ordered Choice Model},
url = {https://arxiv.org/abs/1907.02436},
  author={Lechner, Michael and Okasa, Gabriel},
  journal={arXiv preprint arXiv:1907.02436},
  year={2019}
}
@article{Lechner2019,
author = {Lechner, Michael},
journal = {arXiv preprint arXiv:1812.09487},
title = {{Modified Causal Forests for Estimating Heterogeneous Causal Effects}},
url = {https://arxiv.org/abs/1812.09487},
year = {2019}
}
@incollection{Okasa2018,
  title={Predicting match outcomes in football by an Ordered Forest estimator},
  author={Goller, Daniel and Knaus, Michael C and Lechner, Michael and Okasa, Gabriel},
  booktitle={A Modern Guide to Sports Economics},
  pages={335--355},
  year={2021},
  publisher={Edward Elgar Publishing}
}
@article{Wager2018,
abstract = {Many scientific and engineering challenges---ranging from personalized medicine to customized marketing recommendations---require an understanding of treatment effect heterogeneity. In this paper, we develop a non-parametric causal forest for estimating heterogeneous treatment effects that extends Breiman's widely used random forest algorithm. Given a potential outcomes framework with unconfoundedness, we show that causal forests are pointwise consistent for the true treatment effect, and have an asymptotically Gaussian and centered sampling distribution. We also discuss a practical method for constructing asymptotic confidence intervals for the true treatment effect that are centered at the causal forest estimates. Our theoretical results rely on a generic Gaussian theory for a large family of random forest algorithms, to our knowledge, this is the first set of results that allows any type of random forest, including classification and regression forests, to be used for provably valid statistical inference. In experiments, we find causal forests to be substantially more powerful than classical methods based on nearest-neighbor matching, especially as the number of covariates increases.},
archivePrefix = {arXiv},
arxivId = {1510.04342},
author = {Wager, Stefan and Athey, Susan},
doi = {10.1080/01621459.2017.1319839},
eprint = {1510.04342},
isbn = {0027-8424, 1091-6490},
journal = {Journal of the American Statistical Association},
keywords = {Adaptive nearest neighbors matching,Asymptotic normality,Potential outcomes,Unconfoundedness},
pages = {1228--1242},
pmid = {27382149},
title = {{Estimation and Inference of Heterogeneous Treatment Effects using Random Forests}},
year = {2018}
}
@article{Wright2017,
abstract = {We introduce the C++ application and R package ranger. The software is a fast implementation of random forests for high dimensional data. Ensembles of classification, regression and survival trees are supported. We describe the implementation, provide examples, validate the package with a reference implementation, and compare runtime and memory usage with other implementations. The new software proves to scale best with the number of features, samples, trees, and features tried for splitting. Finally, we show that ranger is the fastest and most memory efficient implementation of random forests to analyze data on the scale of a genome-wide association study.},
author = {Wright, Marvin N. and Ziegler, Andreas},
doi = {10.18637/jss.v077.i01},
issn = {1548-7660},
journal = {Journal of Statistical Software},
keywords = {C++,R,Rcpp,classification,machine learning,random forests,recursive partitioning,survival analysis},
number = {1},
pages = {1--17},
title = {{ranger : A Fast Implementation of Random Forests for High Dimensional Data in C++ and R}},
url = {http://www.jstatsoft.org/v77/i01/},
volume = {77},
year = {2017}
}
@article{Breiman2001,
abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund {\&} R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148--156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
archivePrefix = {arXiv},
arxivId = {http://dx.doi.org/10.1023{\%}2FA{\%}3A1010933404324},
author = {Breiman, L.},
doi = {10.1023/A:1010933404324},
eprint = {/dx.doi.org/10.1023{\%}2FA{\%}3A1010933404324},
isbn = {9781424444427},
issn = {08856125},
journal = {Machine Learning},
keywords = {classification,ensemble,regression},
number = {1},
pages = {5--32},
pmid = {20142443},
primaryClass = {http:},
title = {{Random Forests}},
volume = {45},
year = {2001}
}
